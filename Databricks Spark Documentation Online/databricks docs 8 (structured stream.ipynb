{"cells":[{"cell_type":"code","source":["%fs ls /databricks-datasets/structured-streaming/events/"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%fs head /databricks-datasets/structured-streaming/events/file-0.json"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.types._\n\nval inputPath = \"/databricks-datasets/structured-streaming/events/\"\n\n// Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\nval jsonSchema = new StructType().add(\"time\", TimestampType).add(\"action\", StringType)\n\nval staticInputDF = \n  spark\n    .read\n    .schema(jsonSchema)\n    .json(inputPath)\n\ndisplay(staticInputDF)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions._\n\nval staticCountsDF = \n  staticInputDF\n    .groupBy($\"action\", window($\"time\", \"1 hour\"))\n    .count()   \n\n// Register the DataFrame as table 'static_counts'\nstaticCountsDF.createOrReplaceTempView(\"static_counts\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sql select action, sum(count) as total_count from static_counts group by action"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%sql select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from static_counts order by time, action"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions._\n\n// Similar to definition of staticInputDF above, just using `readStream` instead of `read`\nval streamingInputDF = \n  spark\n    .readStream                       // `readStream` instead of `read` for creating streaming DataFrame\n    .schema(jsonSchema)               // Set the schema of the JSON data\n    .option(\"maxFilesPerTrigger\", 1)  // Treat a sequence of files as a stream by picking one file at a time\n    .json(inputPath)\n\n// Same query as staticInputDF\nval streamingCountsDF = \n  streamingInputDF\n    .groupBy($\"action\", window($\"time\", \"1 hour\"))\n    .count()\n\n// Is this DF actually a streaming DF?\nstreamingCountsDF.isStreaming"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%scala\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")  // keep the size of shuffles small\n\nval query =\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        // memory = store in-memory table (for testing only in Spark 2.0)\n    .queryName(\"counts\")     // counts = name of the in-memory table\n    .outputMode(\"complete\")  // complete = all the counts should be in the table\n    .start()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%sql select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from counts order by time, action"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%scala\nThread.sleep(5000)  // wait a bit more for more data to be computed"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%sql select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from counts order by time, action"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from counts order by time, action"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%sql select action, sum(count) as total_count from counts group by action order by action"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%fs ls /databricks-datasets/structured-streaming/events/"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%fs head /databricks-datasets/structured-streaming/events/file-0.json"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql.types import *\n\ninputPath = \"/databricks-datasets/structured-streaming/events/\"\n\n# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\njsonSchema = StructType([ StructField(\"time\", TimestampType(), True), StructField(\"action\", StringType(), True) ])\n\n# Static DataFrame representing data in the JSON files\nstaticInputDF = (\n  spark\n    .read\n    .schema(jsonSchema)\n    .json(inputPath)\n)\n\ndisplay(staticInputDF)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["from pyspark.sql.functions import *      # for window() function\n\nstaticCountsDF = (\n  staticInputDF\n    .groupBy(\n       staticInputDF.action, \n       window(staticInputDF.time, \"1 hour\"))    \n    .count()\n)\nstaticCountsDF.cache()\n\n# Register the DataFrame as table 'static_counts'\nstaticCountsDF.createOrReplaceTempView(\"static_counts\")\n"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%sql select action, sum(count) as total_count from static_counts group by action"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%sql select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from static_counts order by time, action"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\nstreamingInputDF = (\n  spark\n    .readStream                       \n    .schema(jsonSchema)               # Set the schema of the JSON data\n    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n    .json(inputPath)\n)\n\n# Same query as staticInputDF\nstreamingCountsDF = (                 \n  streamingInputDF\n    .groupBy(\n      streamingInputDF.action, \n      window(streamingInputDF.time, \"1 hour\"))\n    .count()\n)\n\n# Is this DF actually a streaming DF?\nstreamingCountsDF.isStreaming"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n    .queryName(\"counts\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["from time import sleep\nsleep(5)  # wait a bit for computation to start"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["sleep(5)  # wait a bit more for more data to be computed"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["%sql select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from counts order by time, action"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%sql select action, date_format(window.end, \"MMM-dd HH:mm\") as time, count from counts order by time, action"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%sql select action, sum(count) as total_count from counts group by action order by action"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["%fs ls databricks-datasets/adult/adult.data"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%sql DROP TABLE IF EXISTS adult"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["%sql\n\nCREATE TABLE adult (\n  age DOUBLE,\n  workclass STRING,\n  fnlwgt DOUBLE,\n  education STRING,\n  education_num DOUBLE,\n  marital_status STRING,\n  occupation STRING,\n  relationship STRING,\n  race STRING,\n  sex STRING,\n  capital_gain DOUBLE,\n  capital_loss DOUBLE,\n  hours_per_week DOUBLE,\n  native_country STRING,\n  income STRING)\nUSING com.databricks.spark.csv\nOPTIONS (path \"/databricks-datasets/adult/adult.data\", header \"true\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["dataset = spark.table(\"adult\")\ncols = dataset.columns"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["display(dataset)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["###One-Hot Encoding\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n\ncategoricalColumns = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n  # Category Indexing with StringIndexer\n  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n  # Add stages.  These are not run here, but will run all at once later on.\n  stages += [stringIndexer, encoder]"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# Convert label into label indices using the StringIndexer\nlabel_stringIdx = StringIndexer(inputCol = \"income\", outputCol = \"label\")\nstages += [label_stringIdx]"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# Transform all features into a vector using VectorAssembler\nnumericCols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\nassemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# Create a Pipeline.\npipeline = Pipeline(stages=stages)\n# Run the feature transformations.\n#  - fit() computes feature statistics as needed.\n#  - transform() actually transforms the features.\npipelineModel = pipeline.fit(dataset)\ndataset = pipelineModel.transform(dataset)\n\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = dataset.select(selectedcols)\ndisplay(dataset)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["### Randomly split data into training and test sets. set seed for reproducibility\n(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\nprint trainingData.count()\nprint testData.count()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n\n# Create initial LogisticRegression model\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n\n# Train model with Training Data\nlrModel = lr.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# Make predictions on test data using the transform() method.\n# LogisticRegression.transform() will only use the 'features' column.\npredictions = lrModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["predictions.printSchema()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# View model's predictions and probabilities of each prediction class\n# You can select any columns in the above schema to view as well. For example's sake we will choose age & occupation\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["evaluator.getMetricName()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["print lr.explainParams()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# Create ParamGrid for Cross Validation\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["# Use test set here so we can measure the accuracy of our model on new data\npredictions = cvModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["print 'Model Intercept: ', cvModel.bestModel.intercept"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["weights = cvModel.bestModel.weights\n# on Spark 2.X weights are available as ceofficients\n# weights = cvModel.bestModel.coefficients\nweights = map(lambda w: (float(w),), weights)  # convert numpy type to float, and to tuple\nweightsDF = sqlContext.createDataFrame(weights, [\"Feature Weight\"])\ndisplay(weightsDF)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["# View best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier\n\n# Create initial Decision Tree Model\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n\n# Train model with Training Data\ndtModel = dt.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["print \"numNodes = \", dtModel.numNodes\nprint \"depth = \", dtModel.depth"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["# Make predictions on test data using the Transformer.transform() method.\npredictions = dtModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["predictions.printSchema()"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["# View model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["dt.getImpurity()"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["# Create ParamGrid for Cross Validation\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(dt.maxDepth, [1,2,6,10])\n             .addGrid(dt.maxBins, [20,40,80])\n             .build())"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations\ncvModel = cv.fit(trainingData)\n# Takes ~5 minutes"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["print \"numNodes = \", cvModel.bestModel.numNodes\nprint \"depth = \", cvModel.bestModel.depth"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["# Use test set here so we can measure the accuracy of our model on new data\npredictions = cvModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["# View Best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier\n\n# Create an initial RandomForest model.\nrf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n\n# Train model with Training Data\nrfModel = rf.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["# Make predictions on test data using the Transformer.transform() method.\npredictions = rfModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["predictions.printSchema()"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["# View model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Evaluate model\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["# Create ParamGrid for Cross Validation\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(rf.maxDepth, [2, 4, 6])\n             .addGrid(rf.maxBins, [20, 60])\n             .addGrid(rf.numTrees, [5, 20])\n             .build())"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["# Create 5-fold CrossValidator\ncv = CrossValidator(estimator=rf, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n\n# Run cross validations.  This can take about 6 minutes since it is training over 20 trees!\ncvModel = cv.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["# Use test set here so we can measure the accuracy of our model on new data\npredictions = cvModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["# cvModel uses the best model found from the Cross Validation\n# Evaluate best model\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["# View Best model's predictions and probabilities of each prediction class\nselected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["bestModel = cvModel.bestModel"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["# Generate predictions for entire dataset\nfinalPredictions = bestModel.transform(dataset)"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["# Evaluate best model\nevaluator.evaluate(finalPredictions)"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["finalPredictions.createOrReplaceTempView(\"finalPredictions\")"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["%sql\nSELECT occupation, incomePrediction, count(*) AS count\nFROM deploymentTable\nGROUP BY occupation, incomePrediction\nORDER BY occupation"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["%sql\nSELECT age, incomePrediction, count(*) AS count\nFROM deploymentTable\nGROUP BY age, incomePrediction\nORDER BY age"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["# We use the sqlContext.read method to read the data and set a few options:\n#  'format': specifies the Spark CSV data source\n#  'header': set to true to indicate that the first line of the CSV data file is a header\n# The file is called 'hour.csv'.\nif sc.version >= '2.0':\n  # Spark 2.0+ includes CSV as a native Spark SQL datasource.\n  df = sqlContext.read.format('csv').option(\"header\", 'true').load(\"/databricks-datasets/bikeSharing/data-001/hour.csv\")\nelse:\n  # Earlier Spark versions can use the Spark CSV package\n  df = sqlContext.read.format('com.databricks.spark.csv').option(\"header\", 'true').load(\"/databricks-datasets/bikeSharing/data-001/hour.csv\")\n# Calling cache on the DataFrame will make sure we persist it in memory the first time it is used.\n# The following uses will be able to read from memory, instead of re-reading the data from disk.\ndf.cache()"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["print \"Our dataset has %d rows.\" % df.count()"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["df = df.drop(\"instant\").drop(\"dteday\").drop(\"casual\").drop(\"registered\")\ndisplay(df)\n"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["# The following call takes all columns (df.columns) and casts them using Spark SQL to a numeric type (DoubleType).\nfrom pyspark.sql.functions import col  # for indicating a column using a string in the line below\ndf = df.select([col(c).cast(\"double\").alias(c) for c in df.columns])\ndf.printSchema()"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":["# Split the dataset randomly into 70% for training and 30% for testing.\ntrain, test = df.randomSplit([0.7, 0.3])\nprint \"We have %d training examples and %d test examples.\" % (train.count(), test.count())"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["display(train.select(\"hr\", \"cnt\"))\n"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler, VectorIndexer\nfeaturesCols = df.columns\nfeaturesCols.remove('cnt')\n# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\nvectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\")\n# This identifies categorical features and indexes them.\nvectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["from pyspark.ml.regression import GBTRegressor\n# Takes the \"features\" column and learns to predict \"cnt\"\ngbt = GBTRegressor(labelCol=\"cnt\")"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import RegressionEvaluator\n# Define a grid of hyperparameters to test:\n#  - maxDepth: max depth of each decision tree in the GBT ensemble\n#  - maxIter: iterations, i.e., number of trees in each GBT ensemble\n# In this example notebook, we keep these values small.  In practice, to get the highest accuracy, you would likely want to try deeper trees (10 or higher) and more trees in the ensemble (>100).\nparamGrid = ParamGridBuilder()\\\n  .addGrid(gbt.maxDepth, [2, 5])\\\n  .addGrid(gbt.maxIter, [10, 100])\\\n  .build()\n# We define an evaluation metric.  This tells CrossValidator how well we are doing by comparing the true labels with predictions.\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol())\n# Declare the CrossValidator, which runs model tuning for us.\ncv = CrossValidator(estimator=gbt, evaluator=evaluator, estimatorParamMaps=paramGrid)"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["from pyspark.ml import Pipeline\npipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, cv])"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["pipelineModel = pipeline.fit(train)"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["predictions = pipelineModel.transform(test)"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["display(predictions.select(\"cnt\", \"prediction\", *featuresCols))"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"code","source":["rmse = evaluator.evaluate(predictions)\nprint \"RMSE on our test set: %g\" % rmse"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["display(predictions.select(\"hr\", \"prediction\"))"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["displayHTML(sc.wholeTextFiles(\"/databricks-datasets/Rdatasets/data-001/doc/ggplot2/diamonds.html\").take(1)[0][1])"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["%r\ndata(diamonds)"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"code","source":["\n\ndisplayHTML(sc.wholeTextFiles(\"/databricks-datasets/Rdatasets/data-001/doc/ggplot2/diamonds.html\").take(1)[0][1])\n\n# Load data into a Pandas dataframe\nimport pandas\nimport cStringIO\nfrom pyspark.sql import *\nlocalData = sc.wholeTextFiles(\"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\").collect()[0][1]\noutput = cStringIO.StringIO(localData)\npandasData = pandas.read_csv(output)\npandasData = pandasData.iloc[:,1:] # remove line number\n"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["pandasData"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nplt.clf()\nplt.plot(pandasData['carat'], pandasData['price'], '.')\nplt.xlabel('carat')\nplt.ylabel('price')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["# Create this plot by calling display on the Spark DataFrame, clicking the plot icon, selecting Plot Options, and creating a Histogram of 'carat' values.\nsparkDataframe = sqlContext.createDataFrame(pandasData)\ndisplay(sparkDataframe)"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"code","source":["pandasData['cut'] = pandasData['cut'].replace({'Fair':0, 'Good':1, 'Very Good':2, 'Premium':3, 'Ideal':4})\npandasData['color'] = pandasData['color'].replace({'J':0, 'I':1, 'H':2, 'G':3, 'F':4, 'E':5, 'D':6})\npandasData['clarity'] = pandasData['clarity'].replace({'I1':0, 'SI1':1, 'SI2':2, 'VS1':3, 'VS2':4, 'VVS1':5, 'VVS2':6, 'IF':7})\npandasData"],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"code","source":["# Split data into a labels dataframe and a features dataframe\nlabels = pandasData['price'].values\nfeatureNames = ['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z']\nfeatures = pandasData[featureNames].values"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["# Normalize features (columns) to have unit variance\nfrom sklearn.preprocessing import normalize\nfeatures = normalize(features, axis=0)\nfeatures"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"code","source":["# Hold out 30% of the data for testing.  We will use the rest for training.\nfrom sklearn.cross_validation import train_test_split\ntrainingLabels, testLabels, trainingFeatures, testFeatures = train_test_split(labels, features, test_size=0.3)\nntrain, ntest = len(trainingLabels), len(testLabels)\nprint 'Split data randomly into 2 sets: %d training and %d test instances.' % (ntrain, ntest)"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"code","source":["# Train a model with fixed hyperparameters, and print out the intercept and coefficients.\nfrom sklearn import linear_model\norigAlpha = 0.5 # \"alpha\" is the regularization hyperparameter\norigClf = linear_model.Ridge(alpha=origAlpha)\norigClf.fit(features, labels)\nprint 'Trained model with fixed alpha = %g' % origAlpha\nprint '  Model intercept: %g' % origClf.intercept_\nprint '  Model coefficients:'\nfor i in range(len(featureNames)):\n  print '    %g\\t%s' % (origClf.coef_[i], featureNames[i])"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"code","source":["# Score the initial model.  It does not do that well.\norigScore = origClf.score(trainingFeatures, trainingLabels)\norigScore"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"code","source":["# We use scikit-learn's cross_validation module, which helps split our data randomly into k equal-size parts (\"folds\").\nfrom sklearn import cross_validation\nnumFolds = 3 # You may want to use more (10 or so) in practice\nkf = cross_validation.KFold(ntrain, n_folds=numFolds)\n\n"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"code","source":["# \"alphas\" is a list of hyperparameter values to test\nalphas = [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n# Create a list of tasks to distribute\ntasks = []\nfor alpha in alphas:\n  for fold in range(numFolds):\n    tasks = tasks + [(alpha, fold)]"],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"code","source":["# Create an RDD of tasks.  We set the number of partitions equal to the number of tasks to ensure maximum parallelism.\ntasksRDD = sc.parallelize(tasks, numSlices = len(tasks))"],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"code","source":["trainingFeaturesBroadcast = sc.broadcast(trainingFeatures)\ntrainingLabelsBroadcast = sc.broadcast(trainingLabels)"],"metadata":{},"outputs":[],"execution_count":113},{"cell_type":"code","source":["def trainOneModel(alpha, fold):\n  \"\"\"\n  Given 1 task (1 hyperparameter alpha value + 1 fold index), train the corresponding model.\n  Return: model, score on the fold's test data, task info.\n  \"\"\"\n  # Extract indices for this fold\n  trainIndex, valIndex = [], []\n  fold_ = 0 # index into folds 'kf'\n  for trainIndex_, valIndex_ in kf:\n    if fold_ == fold:\n      trainIndex, valIndex = trainIndex_, valIndex_\n      break\n    fold_ += 1\n  # Get training data from the broadcast variables\n  localTrainingFeatures = trainingFeaturesBroadcast.value\n  localTrainingLabels = trainingLabelsBroadcast.value\n  X_train, X_val = localTrainingFeatures[trainIndex], localTrainingFeatures[valIndex]\n  Y_train, Y_val = localTrainingLabels[trainIndex], localTrainingLabels[valIndex]\n  # Train the model, and score it\n  clf = linear_model.Ridge(alpha=alpha)\n  clf.fit(X_train, Y_train)\n  score = clf.score(X_val, Y_val)\n  return clf, score, alpha, fold"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"code","source":["# LEARN!  We now map our tasks RDD and apply the training function to each task.\n# After we call an action (\"count\") on the results, the actual training is executed.\ntrainedModelAndScores = tasksRDD.map(lambda alpha_fold: trainOneModel(alpha_fold[0], alpha_fold[1]))\ntrainedModelAndScores.cache()\ntrainedModelAndScores.count()"],"metadata":{},"outputs":[],"execution_count":115},{"cell_type":"code","source":["# Since we are done with our broadcast variables, we can clean them up.\n# (This will happen automatically, but we can make it happen earlier by explicitly unpersisting the broadcast variables.\ntrainingFeaturesBroadcast.unpersist()\ntrainingLabelsBroadcast.unpersist()"],"metadata":{},"outputs":[],"execution_count":116},{"cell_type":"code","source":["# Collect the results.\nallScores = trainedModelAndScores.map(lambda x: (x[1], x[2], x[3])).collect()\n# Average scores over folds\navgScores = dict(map(lambda alpha: (alpha, 0.0), alphas))\nfor score, alpha, fold in allScores:\n  avgScores[alpha] += score\nfor alpha in alphas:\n  avgScores[alpha] /= numFolds\navgScores"],"metadata":{},"outputs":[],"execution_count":117},{"cell_type":"code","source":["# Find best score\nbestAlpha = -1\nbestScore = -1\nfor alpha in alphas:\n  if avgScores[alpha] > bestScore:\n    bestAlpha = alpha\n    bestScore = avgScores[alpha]\nprint 'Found best alpha: %g, which gives score: %g' % (bestAlpha, bestScore)"],"metadata":{},"outputs":[],"execution_count":118},{"cell_type":"code","source":["# Use Databricks' display() function to plot the scores vs. alpha.  We use a namedtuple to tell Databricks names for the columns (alpha and the score).\nimport numpy\nfrom collections import namedtuple\nScore = namedtuple('Score', 'log_alpha score')\ndisplay(map(lambda alpha: Score(float(numpy.log(alpha + 0.00000001)), float(avgScores[alpha])), avgScores))"],"metadata":{},"outputs":[],"execution_count":119},{"cell_type":"code","source":["# Use bestAlpha, and train a final model.\ntunedClf = linear_model.Ridge(alpha=bestAlpha)\ntunedClf.fit(trainingFeatures, trainingLabels)"],"metadata":{},"outputs":[],"execution_count":120},{"cell_type":"code","source":["origTrainingScore, origTestScore = origClf.score(trainingFeatures, trainingLabels), origClf.score(testFeatures, testLabels)\ntunedTrainingScore, tunedTestScore = tunedClf.score(trainingFeatures, trainingLabels), tunedClf.score(testFeatures, testLabels)\nprint 'Compare original model (without hyperparameter tuning) and final model (with tuning) on test data\\n'\nprint 'Model   \\tAlpha\\tTraining   \\tTest'\nprint 'Original\\t%g\\t%g\\t%g' % (origAlpha, origTrainingScore, origTestScore)\nprint 'Tuned   \\t%g\\t%g\\t%g' % (bestAlpha, tunedTrainingScore, tunedTestScore)"],"metadata":{},"outputs":[],"execution_count":121},{"cell_type":"code","source":["print 'Tuned model with best alpha = %g' % bestAlpha\nprint '  Model intercept: %g' % tunedClf.intercept_\nprint '  Model coefficients:'\nfor i in range(len(featureNames)):\n  print '    %g\\t%s' % (tunedClf.coef_[i], featureNames[i])"],"metadata":{},"outputs":[],"execution_count":122},{"cell_type":"code","source":["# Convert the scikit-learn model into an equivalent MLlib model\nfrom pyspark.mllib.regression import LinearRegressionModel\nmllibModel = LinearRegressionModel(tunedClf.coef_, tunedClf.intercept_)\nmllibModel"],"metadata":{},"outputs":[],"execution_count":123},{"cell_type":"code","source":["# Demonstrate that the models compute the same predictions\nsklearnPredictions = tunedClf.predict(testFeatures)\nmllibPredictions = numpy.array(map(lambda x: mllibModel.predict(x), testFeatures))\ndifferences = sklearnPredictions - mllibPredictions\nsumSquaredDifferences = sum(differences * differences)\nprint 'Total difference between scikit-learn and MLlib model predictions: %g' % sumSquaredDifferences"],"metadata":{},"outputs":[],"execution_count":124},{"cell_type":"code","source":["import org.apache.spark.h2o._\nval h2oConf = new H2OConf(sc).set(\"spark.ui.enabled\", \"false\")\nval h2oContext = H2OContext.getOrCreate(sc, h2oConf)"],"metadata":{},"outputs":[],"execution_count":125},{"cell_type":"code","source":["%fs ls /databricks-datasets/sms_spam_collection/data-001"],"metadata":{},"outputs":[],"execution_count":126},{"cell_type":"code","source":["%sql \nDROP TABLE IF EXISTS smsData;\nCREATE TABLE smsData(hamorspam string, msg string)\nUSING com.databricks.spark.csv\nOPTIONS (path \"dbfs:/databricks-datasets/sms_spam_collection/data-001/smsData.csv\", delimiter \"\\t\", inferSchema \"true\")"],"metadata":{},"outputs":[],"execution_count":127},{"cell_type":"code","source":["spark.table(\"smsData\").printSchema"],"metadata":{},"outputs":[],"execution_count":128},{"cell_type":"code","source":["%sql select * from smsData"],"metadata":{},"outputs":[],"execution_count":129},{"cell_type":"code","source":["%sql select  hamorspam, count(1) cnt from smsdata group by hamorspam"],"metadata":{},"outputs":[],"execution_count":130},{"cell_type":"code","source":["%scala\ncase class sms(id : Long, hamorspam : String, msg : String)\nval smsData = spark.table(\"smsData\").rdd.zipWithIndex.map{x => sms(x._2, x._1.getString(0), x._1.getString(1))}.toDF()"],"metadata":{},"outputs":[],"execution_count":131},{"cell_type":"code","source":["%scala\nspark.udf.register(\"removeSpecialChars\", (s : String) => {\n    val ignoredChars = Seq(',', ':', ';', '/', '<', '>', '\"', '.', '(', ')', '?', '-', '\\'','!','0', '1')\n    var result : String = s\n    for( c <- ignoredChars) {\n         result = result.replace(c, ' ')\n    }\n    result\n })\n \n val smsDataNoSpecialChars = smsData.selectExpr(\"*\", \"removeSpecialChars(msg) as msg_no_special_chars\")"],"metadata":{},"outputs":[],"execution_count":132},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.feature.{Tokenizer, StopWordsRemover, IDF, VectorAssembler, HashingTF, StringIndexer}\nimport org.apache.spark.ml.Pipeline\n\n\n \n\nval tok = new Tokenizer()\n  .setInputCol(\"msg_no_special_chars\")\n  .setOutputCol(\"words\")\nval sw = new StopWordsRemover()\n  .setInputCol(\"words\")\n  .setOutputCol(\"filtered_words\")\nval tf = new HashingTF()\n  .setInputCol(\"filtered_words\")\n  .setOutputCol(\"tf\")\n  .setNumFeatures(10000)\nval idf = new IDF()\n  .setInputCol(\"tf\")\n  .setOutputCol(\"tf_idf\")\nval labeler = new StringIndexer()\n  .setInputCol(\"hamorspam\")\n  .setOutputCol(\"label\")\nval assembler = new VectorAssembler()\n  .setInputCols(Array(\"tf_idf\"))\n  .setOutputCol(\"features\")\n\nval pipeline = new Pipeline().setStages(Array(tok, sw, tf, idf, labeler, assembler))"],"metadata":{},"outputs":[],"execution_count":133},{"cell_type":"code","source":["%scala\nval sparkDataPrepModel = pipeline.fit(smsDataNoSpecialChars)\nval sparkPreparedData = sparkDataPrepModel.transform(smsDataNoSpecialChars)"],"metadata":{},"outputs":[],"execution_count":134},{"cell_type":"code","source":["%scala\ndisplay(sparkPreparedData)"],"metadata":{},"outputs":[],"execution_count":135},{"cell_type":"code","source":["%scala\nimport h2oContext._\nimport h2oContext.implicits._\nimport hex.deeplearning.{DeepLearningModel, DeepLearning}\nimport hex.deeplearning.DeepLearningModel.DeepLearningParameters"],"metadata":{},"outputs":[],"execution_count":136},{"cell_type":"code","source":["%scala\nval Array(trainDf, testDf) = sparkPreparedData.randomSplit(Array(0.7, 0.3))"],"metadata":{},"outputs":[],"execution_count":137},{"cell_type":"code","source":["%scala\nval train = h2oContext.asH2OFrame(trainDf.select(\"id\", \"label\", \"features\"))\nval valid = h2oContext.asH2OFrame(testDf.select(\"id\", \"label\", \"features\"))"],"metadata":{},"outputs":[],"execution_count":138},{"cell_type":"code","source":["%scala\nval dlParams = new DeepLearningParameters()\n    dlParams._train = train\n    dlParams._valid = valid\n    dlParams._response_column = 'target\n    dlParams._epochs = 10\n    dlParams._l1 = 0.001\n    dlParams._hidden = Array[Int](200, 200)\n    dlParams._response_column = \"label\"\n    dlParams._ignored_columns = Array(\"id\")\n\nval dl = new DeepLearning(dlParams)\nval dlModel = dl.trainModel.get"],"metadata":{},"outputs":[],"execution_count":139},{"cell_type":"code","source":["%scala\ndlModel"],"metadata":{},"outputs":[],"execution_count":140},{"cell_type":"code","source":["%scala\nimport org.apache.spark.ml.Transformer\nimport org.apache.spark.ml.param.{Param, ParamMap}\nimport org.apache.spark.sql.{DataFrame, Dataset}\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions.{col, explode, udf}\nimport org.apache.spark.sql.types.{DataTypes, StructType}\n\n/**\n * An example demonstrating how to write a custom Transformer in a 3rd-party application.\n * This example intentionally avoids using any private Spark APIs.\n *\n * @param uid  All types inheriting from `Identifiable` require a `uid`.\n *             This includes Transformers, Estimators, and Models.\n */\nclass MyFlatMapTransformer(override val uid: String) extends Transformer {\n\n  // Transformer Params\n  // Defining a Param requires 3 elements:\n  //  - Param definition\n  //  - Param getter method\n  //  - Param setter method\n  // (The getter and setter are technically not required, but they are nice standards to follow.)\n\n  /**\n   * Param for input column name.\n   */\n  final val inputCol: Param[String] = new Param[String](this, \"inputCol\", \"input column name\")\n\n  final def getInputCol: String = $(inputCol)\n\n  final def setInputCol(value: String): MyFlatMapTransformer = set(inputCol, value)\n\n  /**\n   * Param for output column name.\n   */\n  final val outputCol: Param[String] = new Param[String](this, \"outputCol\", \"output column name\")\n\n  final def getOutputCol: String = $(outputCol)\n\n  final def setOutputCol(value: String): MyFlatMapTransformer = set(outputCol, value)\n\n  // (Optional) You can set defaults for Param values if you like.\n  setDefault(inputCol -> \"myInputCol\", outputCol -> \"myOutputCol\")\n\n  // Transformer requires 3 methods:\n  //  - transform\n  //  - transformSchema\n  //  - copy\n\n  // Our flatMap will split strings by commas.\n  private val myFlatMapFunction: String => Seq[String] = { input: String =>\n    input.split(\",\")\n  }\n\n  /**\n   * This method implements the main transformation.\n   * Its required semantics are fully defined by the method API: take a Dataset or DataFrame,\n   * and return a DataFrame.\n   *\n   * Most Transformers are 1-to-1 row mappings which add one or more new columns and do not\n   * remove any columns.  However, this restriction is not required.  This example does a flatMap,\n   * so we could either (a) drop other columns or (b) keep other columns, making copies of values\n   * in each row as it expands to multiple rows in the flatMap.  We do (a) for simplicity.\n   */\n  override def transform(dataset: Dataset[_]): DataFrame = {\n    val flatMapUdf = udf(myFlatMapFunction)\n    dataset.select(explode(flatMapUdf(col($(inputCol)))).as($(outputCol)))\n  }\n\n  /**\n   * Check transform validity and derive the output schema from the input schema.\n   *\n   * We check validity for interactions between parameters during `transformSchema` and\n   * raise an exception if any parameter value is invalid. Parameter value checks which\n   * do not depend on other parameters are handled by `Param.validate()`.\n   *\n   * Typical implementation should first conduct verification on schema change and parameter\n   * validity, including complex parameter interaction checks.\n   */\n  override def transformSchema(schema: StructType): StructType = {\n    // Validate input type.\n    // Input type validation is technically optional, but it is a good practice since it catches\n    // schema errors early on.\n    val actualDataType = schema($(inputCol)).dataType\n    require(actualDataType.equals(DataTypes.StringType),\n      s\"Column ${$(inputCol)} must be StringType but was actually $actualDataType.\")\n\n    // Compute output type.\n    // This is important to do correctly when plugging this Transformer into a Pipeline,\n    // where downstream Pipeline stages may expect use this Transformer's output as their input.\n    DataTypes.createStructType(\n      Array(\n        DataTypes.createStructField($(outputCol), DataTypes.StringType, false)\n      )\n    )\n  }\n\n  /**\n   * Creates a copy of this instance.\n   * Requirements:\n   *  - The copy must have the same UID.\n   *  - The copy must have the same Params, with some possibly overwritten by the `extra`\n   *    argument.\n   *  - This should do a deep copy of any data members which are mutable.  That said,\n   *    Transformers should generally be immutable (except for Params), so the `defaultCopy`\n   *    method often suffices.\n   * @param extra  Param values which will overwrite Params in the copy.\n   */\n  override def copy(extra: ParamMap): Transformer = defaultCopy(extra)\n}"],"metadata":{},"outputs":[],"execution_count":141},{"cell_type":"code","source":["%scala\nval data = spark.createDataFrame(Seq(\n  (\"hi,there\", 1),\n  (\"a,b,c\", 2),\n  (\"no\", 3)\n)).toDF(\"myInputCol\", \"id\")\nval myTransformer = new MyFlatMapTransformer(\"myFlatMapper\")\nprintln(s\"Original data has ${data.count()} rows.\")"],"metadata":{},"outputs":[],"execution_count":142},{"cell_type":"code","source":["%scala\ndisplay(data)"],"metadata":{},"outputs":[],"execution_count":143},{"cell_type":"code","source":["%scala\nval output = myTransformer.transform(data)\nprintln(s\"Output data has ${output.count()} rows.\")"],"metadata":{},"outputs":[],"execution_count":144},{"cell_type":"code","source":["%scala\ndisplay(output)"],"metadata":{},"outputs":[],"execution_count":145},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":146}],"metadata":{"name":"databricks docs 8 (structured stream","notebookId":3117922773650743},"nbformat":4,"nbformat_minor":0}
