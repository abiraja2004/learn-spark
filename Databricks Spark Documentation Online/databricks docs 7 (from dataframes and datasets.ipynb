{"cells":[{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.types._                         // include the Spark Types to define our schema\nimport org.apache.spark.sql.functions._                     // include the Spark helper functions\n\nval jsonSchema = new StructType()\n        .add(\"battery_level\", LongType)\n        .add(\"c02_level\", LongType)\n        .add(\"cca3\",StringType)\n        .add(\"cn\", StringType)\n        .add(\"device_id\", LongType)\n        .add(\"device_type\", StringType)\n        .add(\"signal\", LongType)\n        .add(\"ip\", StringType)\n        .add(\"temp\", LongType)\n        .add(\"timestamp\", TimestampType)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%scala\n// define a case class\ncase class DeviceData (id: Int, device: String)\n// create some sample data\nval eventsDS = Seq (\n (0, \"\"\"{\"device_id\": 0, \"device_type\": \"sensor-ipad\", \"ip\": \"68.161.225.1\", \"cca3\": \"USA\", \"cn\": \"United States\", \"temp\": 25, \"signal\": 23, \"battery_level\": 8, \"c02_level\": 917, \"timestamp\" :1475600496 }\"\"\"),\n (1, \"\"\"{\"device_id\": 1, \"device_type\": \"sensor-igauge\", \"ip\": \"213.161.254.1\", \"cca3\": \"NOR\", \"cn\": \"Norway\", \"temp\": 30, \"signal\": 18, \"battery_level\": 6, \"c02_level\": 1413, \"timestamp\" :1475600498 }\"\"\"),\n (2, \"\"\"{\"device_id\": 2, \"device_type\": \"sensor-ipad\", \"ip\": \"88.36.5.1\", \"cca3\": \"ITA\", \"cn\": \"Italy\", \"temp\": 18, \"signal\": 25, \"battery_level\": 5, \"c02_level\": 1372, \"timestamp\" :1475600500 }\"\"\"),\n (3, \"\"\"{\"device_id\": 3, \"device_type\": \"sensor-inest\", \"ip\": \"66.39.173.154\", \"cca3\": \"USA\", \"cn\": \"United States\", \"temp\": 47, \"signal\": 12, \"battery_level\": 1, \"c02_level\": 1447, \"timestamp\" :1475600502 }\"\"\"),\n(4, \"\"\"{\"device_id\": 4, \"device_type\": \"sensor-ipad\", \"ip\": \"203.82.41.9\", \"cca3\": \"PHL\", \"cn\": \"Philippines\", \"temp\": 29, \"signal\": 11, \"battery_level\": 0, \"c02_level\": 983, \"timestamp\" :1475600504 }\"\"\"),\n(5, \"\"\"{\"device_id\": 5, \"device_type\": \"sensor-istick\", \"ip\": \"204.116.105.67\", \"cca3\": \"USA\", \"cn\": \"United States\", \"temp\": 50, \"signal\": 16, \"battery_level\": 8, \"c02_level\": 1574, \"timestamp\" :1475600506 }\"\"\"),\n(6, \"\"\"{\"device_id\": 6, \"device_type\": \"sensor-ipad\", \"ip\": \"220.173.179.1\", \"cca3\": \"CHN\", \"cn\": \"China\", \"temp\": 21, \"signal\": 18, \"battery_level\": 9, \"c02_level\": 1249, \"timestamp\" :1475600508 }\"\"\"),\n(7, \"\"\"{\"device_id\": 7, \"device_type\": \"sensor-ipad\", \"ip\": \"118.23.68.227\", \"cca3\": \"JPN\", \"cn\": \"Japan\", \"temp\": 27, \"signal\": 15, \"battery_level\": 0, \"c02_level\": 1531, \"timestamp\" :1475600512 }\"\"\"),\n(8 ,\"\"\" {\"device_id\": 8, \"device_type\": \"sensor-inest\", \"ip\": \"208.109.163.218\", \"cca3\": \"USA\", \"cn\": \"United States\", \"temp\": 40, \"signal\": 16, \"battery_level\": 9, \"c02_level\": 1208, \"timestamp\" :1475600514 }\"\"\"),\n(9,\"\"\"{\"device_id\": 9, \"device_type\": \"sensor-ipad\", \"ip\": \"88.213.191.34\", \"cca3\": \"ITA\", \"cn\": \"Italy\", \"temp\": 19, \"signal\": 11, \"battery_level\": 0, \"c02_level\": 1171, \"timestamp\" :1475600516 }\"\"\"),\n(10,\"\"\"{\"device_id\": 10, \"device_type\": \"sensor-igauge\", \"ip\": \"68.28.91.22\", \"cca3\": \"USA\", \"cn\": \"United States\", \"temp\": 32, \"signal\": 26, \"battery_level\": 7, \"c02_level\": 886, \"timestamp\" :1475600518 }\"\"\"),\n(11,\"\"\"{\"device_id\": 11, \"device_type\": \"sensor-ipad\", \"ip\": \"59.144.114.250\", \"cca3\": \"IND\", \"cn\": \"India\", \"temp\": 46, \"signal\": 25, \"battery_level\": 4, \"c02_level\": 863, \"timestamp\" :1475600520 }\"\"\"),\n(12, \"\"\"{\"device_id\": 12, \"device_type\": \"sensor-igauge\", \"ip\": \"193.156.90.200\", \"cca3\": \"NOR\", \"cn\": \"Norway\", \"temp\": 18, \"signal\": 26, \"battery_level\": 8, \"c02_level\": 1220, \"timestamp\" :1475600522 }\"\"\"),\n(13, \"\"\"{\"device_id\": 13, \"device_type\": \"sensor-ipad\", \"ip\": \"67.185.72.1\", \"cca3\": \"USA\", \"cn\": \"United States\", \"temp\": 34, \"signal\": 20, \"battery_level\": 8, \"c02_level\": 1504, \"timestamp\" :1475600524 }\"\"\"),\n(14, \"\"\"{\"device_id\": 14, \"device_type\": \"sensor-inest\", \"ip\": \"68.85.85.106\", \"cca3\": \"USA\", \"cn\": \"United States\", \"temp\": 39, \"signal\": 17, \"battery_level\": 8, \"c02_level\": 831, \"timestamp\" :1475600526 }\"\"\"),\n(15, \"\"\"{\"device_id\": 15, \"device_type\": \"sensor-ipad\", \"ip\": \"161.188.212.254\", \"cca3\": \"USA\", \"cn\": \"United States\", \"temp\": 27, \"signal\": 26, \"battery_level\": 5, \"c02_level\": 1378, \"timestamp\" :1475600528 }\"\"\"),\n(16, \"\"\"{\"device_id\": 16, \"device_type\": \"sensor-igauge\", \"ip\": \"221.3.128.242\", \"cca3\": \"CHN\", \"cn\": \"China\", \"temp\": 10, \"signal\": 24, \"battery_level\": 6, \"c02_level\": 1423, \"timestamp\" :1475600530 }\"\"\"),\n(17, \"\"\"{\"device_id\": 17, \"device_type\": \"sensor-ipad\", \"ip\": \"64.124.180.215\", \"cca3\": \"USA\", \"cn\": \"United States\", \"temp\": 38, \"signal\": 17, \"battery_level\": 9, \"c02_level\": 1304, \"timestamp\" :1475600532 }\"\"\"),\n(18, \"\"\"{\"device_id\": 18, \"device_type\": \"sensor-igauge\", \"ip\": \"66.153.162.66\", \"cca3\": \"USA\", \"cn\": \"United States\", \"temp\": 26, \"signal\": 10, \"battery_level\": 0, \"c02_level\": 902, \"timestamp\" :1475600534 }\"\"\"),\n(19, \"\"\"{\"device_id\": 19, \"device_type\": \"sensor-ipad\", \"ip\": \"193.200.142.254\", \"cca3\": \"AUT\", \"cn\": \"Austria\", \"temp\": 32, \"signal\": 27, \"battery_level\": 5, \"c02_level\": 1282, \"timestamp\" :1475600536 }\"\"\")).toDF(\"id\", \"device\").as[DeviceData]"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%scala\ndisplay(eventsDS)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%scala\neventsDS.printSchema"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%scala\nval eventsFromJSONDF = Seq (\n (0, \"\"\"{\"device_id\": 0, \"device_type\": \"sensor-ipad\", \"ip\": \"68.161.225.1\", \"cca3\": \"USA\", \"cn\": \"United States\", \"temp\": 25, \"signal\": 23, \"battery_level\": 8, \"c02_level\": 917, \"timestamp\" :1475600496 }\"\"\"),\n (1, \"\"\"{\"device_id\": 1, \"device_type\": \"sensor-igauge\", \"ip\": \"213.161.254.1\", \"cca3\": \"NOR\", \"cn\": \"Norway\", \"temp\": 30, \"signal\": 18, \"battery_level\": 6, \"c02_level\": 1413, \"timestamp\" :1475600498 }\"\"\"),\n (2, \"\"\"{\"device_id\": 2, \"device_type\": \"sensor-ipad\", \"ip\": \"88.36.5.1\", \"cca3\": \"ITA\", \"cn\": \"Italy\", \"temp\": 18, \"signal\": 25, \"battery_level\": 5, \"c02_level\": 1372, \"timestamp\" :1475600500 }\"\"\"),\n (3, \"\"\"{\"device_id\": 3, \"device_type\": \"sensor-inest\", \"ip\": \"66.39.173.154\", \"cca3\": \"USA\", \"cn\": \"United States\", \"temp\": 47, \"signal\": 12, \"battery_level\": 1, \"c02_level\": 1447, \"timestamp\" :1475600502 }\"\"\"),\n(4, \"\"\"{\"device_id\": 4, \"device_type\": \"sensor-ipad\", \"ip\": \"203.82.41.9\", \"cca3\": \"PHL\", \"cn\": \"Philippines\", \"temp\": 29, \"signal\": 11, \"battery_level\": 0, \"c02_level\": 983, \"timestamp\" :1475600504 }\"\"\"),\n(5, \"\"\"{\"device_id\": 5, \"device_type\": \"sensor-istick\", \"ip\": \"204.116.105.67\", \"cca3\": \"USA\", \"cn\": \"United States\", \"temp\": 50, \"signal\": 16, \"battery_level\": 8, \"c02_level\": 1574, \"timestamp\" :1475600506 }\"\"\"),\n(6, \"\"\"{\"device_id\": 6, \"device_type\": \"sensor-ipad\", \"ip\": \"220.173.179.1\", \"cca3\": \"CHN\", \"cn\": \"China\", \"temp\": 21, \"signal\": 18, \"battery_level\": 9, \"c02_level\": 1249, \"timestamp\" :1475600508 }\"\"\"),\n(7, \"\"\"{\"device_id\": 7, \"device_type\": \"sensor-ipad\", \"ip\": \"118.23.68.227\", \"cca3\": \"JPN\", \"cn\": \"Japan\", \"temp\": 27, \"signal\": 15, \"battery_level\": 0, \"c02_level\": 1531, \"timestamp\" :1475600512 }\"\"\"),\n(8 ,\"\"\" {\"device_id\": 8, \"device_type\": \"sensor-inest\", \"ip\": \"208.109.163.218\", \"cca3\": \"USA\", \"cn\": \"United States\", \"temp\": 40, \"signal\": 16, \"battery_level\": 9, \"c02_level\": 1208, \"timestamp\" :1475600514 }\"\"\"),\n(9,\"\"\"{\"device_id\": 9, \"device_type\": \"sensor-ipad\", \"ip\": \"88.213.191.34\", \"cca3\": \"ITA\", \"cn\": \"Italy\", \"temp\": 19, \"signal\": 11, \"battery_level\": 0, \"c02_level\": 1171, \"timestamp\" :1475600516 }\"\"\")).toDF(\"id\", \"json\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%scala\ndisplay(eventsFromJSONDF)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%scala\nval jsDF = eventsFromJSONDF.select($\"id\", get_json_object($\"json\", \"$.device_type\").alias(\"device_type\"),\n                                          get_json_object($\"json\", \"$.ip\").alias(\"ip\"),\n                                         get_json_object($\"json\", \"$.cca3\").alias(\"cca3\"))\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%scala\ndisplay(jsDF)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%scala\nval devicesDF = eventsDS.select(from_json($\"device\", jsonSchema) as \"devices\")\n.select($\"devices.*\")\n.filter($\"devices.temp\" > 10 and $\"devices.signal\" > 15)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%scala\ndisplay(devicesDF)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%scala\nval devicesUSDF = devicesDF.select($\"*\").where($\"cca3\" === \"USA\").orderBy($\"signal\".desc, $\"temp\".desc)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%scala\ndisplay(devicesUSDF)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%scala\nval stringJsonDF = eventsDS.select(to_json(struct($\"*\"))).toDF(\"devices\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["%scala\ndisplay(stringJsonDF)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%scala\nval stringsDF = eventsDS.selectExpr(\"CAST(id AS INT)\", \"CAST(device AS STRING)\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%scala\nstringsDF.printSchema"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%scala\ndisplay(stringsDF)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%scala\ndisplay(devicesDF.selectExpr(\"c02_level\", \"round(c02_level/temp) as ratio_c02_temperature\").orderBy($\"ratio_c02_temperature\" desc))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%scala\ndevicesDF.createOrReplaceTempView(\"devicesDFT\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%sql select c02_level, \n        round(c02_level/temp) as ratio_c02_temperature \n        from devicesDFT\n        order by ratio_c02_temperature desc\n"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%scala\nstringJsonDF\n  .write\n  .mode(\"overwrite\")\n  .format(\"parquet\")\n  .save(\"/tmp/jules\")"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["%fs ls /tmp/jules"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%scala\nval parquetDF = spark.read.parquet(\"/tmp/jules\")"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["%scala\nparquetDF.printSchema"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["%scala\ndisplay(parquetDF)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.types._\n\nval schema = new StructType()\n  .add(\"dc_id\", StringType)                               // data center where data was posted to Kafka cluster\n  .add(\"source\",                                          // info about the source of alarm\n    MapType(                                              // define this as a Map(Key->value)\n      StringType,\n      new StructType()\n      .add(\"description\", StringType)\n      .add(\"ip\", StringType)\n      .add(\"id\", LongType)\n      .add(\"temp\", LongType)\n      .add(\"c02_level\", LongType)\n      .add(\"geo\", \n         new StructType()\n          .add(\"lat\", DoubleType)\n          .add(\"long\", DoubleType)\n        )\n      )\n    )"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%scala\nval dataDS = Seq(\"\"\"\n{\n\"dc_id\": \"dc-101\",\n\"source\": {\n    \"sensor-igauge\": {\n      \"id\": 10,\n      \"ip\": \"68.28.91.22\",\n      \"description\": \"Sensor attached to the container ceilings\",\n      \"temp\":35,\n      \"c02_level\": 1475,\n      \"geo\": {\"lat\":38.00, \"long\":97.00}                        \n    },\n    \"sensor-ipad\": {\n      \"id\": 13,\n      \"ip\": \"67.185.72.1\",\n      \"description\": \"Sensor ipad attached to carbon cylinders\",\n      \"temp\": 34,\n      \"c02_level\": 1370,\n      \"geo\": {\"lat\":47.41, \"long\":-122.00}\n    },\n    \"sensor-inest\": {\n      \"id\": 8,\n      \"ip\": \"208.109.163.218\",\n      \"description\": \"Sensor attached to the factory ceilings\",\n      \"temp\": 40,\n      \"c02_level\": 1346,\n      \"geo\": {\"lat\":33.61, \"long\":-111.89}\n    },\n    \"sensor-istick\": {\n      \"id\": 5,\n      \"ip\": \"204.116.105.67\",\n      \"description\": \"Sensor embedded in exhaust pipes in the ceilings\",\n      \"temp\": 40,\n      \"c02_level\": 1574,\n      \"geo\": {\"lat\":35.93, \"long\":-85.46}\n    }\n  }\n}\"\"\").toDS()\n// should only be one item\ndataDS.count()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["%scala\ndisplay(dataDS)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%scala\nval df = spark                  // spark session \n.read                           // get DataFrameReader\n.schema(schema)                 // use the defined schema above and read format as JSON\n.json(dataDS.rdd)               // RDD[String]"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["%scala\ndf.printSchema"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["%scala\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["%scala\nval explodedDF = df.select($\"dc_id\", explode($\"source\"))\ndisplay(explodedDF)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["%scala\nexplodedDF.printSchema"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["%scala\ncase class DeviceAlert(dcId: String, deviceType:String, ip:String, deviceId:Long, temp:Long, c02_level: Long, lat: Double, lon: Double)\n//access all values using getItem() method on value, by providing the \"key,\" which is attribute in our JSON object.\nval notifydevicesDS = explodedDF.select( $\"dc_id\" as \"dcId\",\n                        $\"key\" as \"deviceType\",\n                        'value.getItem(\"ip\") as 'ip,\n                        'value.getItem(\"id\") as 'deviceId,\n                        'value.getItem(\"c02_level\") as 'c02_level,\n                        'value.getItem(\"temp\") as 'temp,\n                        'value.getItem(\"geo\").getItem(\"lat\") as 'lat,                //note embedded level requires yet another level of fetching.\n                        'value.getItem(\"geo\").getItem(\"long\") as 'lon)\n                        .as[DeviceAlert]  // return as a Dataset"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["%scala\nnotifydevicesDS.printSchema"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["%scala\ndisplay(notifydevicesDS)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["%scala\n// define a Scala Notification Object\nobject DeviceNOCAlerts {\n\n  def sendTwilio(message: String): Unit = {\n    //TODO: fill as necessary\n    println(\"Twilio:\" + message)\n  }\n\n  def sendSNMP(message: String): Unit = {\n    //TODO: fill as necessary\n    println(\"SNMP:\" + message)\n  }\n  \n  def sendKafka(message: String): Unit = {\n    //TODO: fill as necessary\n     println(\"KAFKA:\" + message)\n  }\n}\ndef logAlerts(log: java.io.PrintStream = Console.out, deviceAlert: DeviceAlert, alert: String, notify: String =\"twilio\"): Unit = {\nval message = \"[***ALERT***: %s; data_center: %s, device_name: %s, temperature: %d; device_id: %d ; ip: %s ; c02: %d]\" format(alert, deviceAlert.dcId, deviceAlert.deviceType,deviceAlert.temp, deviceAlert.deviceId, deviceAlert.ip, deviceAlert.c02_level)                                                                                                                                                                                                                                                            \n  //default log to Stderr/Stdout\n  log.println(message)\n  // use an appropriate notification method\n  val notifyFunc = notify match {\n      case \"twilio\" => DeviceNOCAlerts.sendTwilio _\n      case \"snmp\" => DeviceNOCAlerts.sendSNMP _\n      case \"kafka\" => DeviceNOCAlerts.sendKafka _\n  }\n  //send the appropriate alert\n  notifyFunc(message)\n}"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["%scala\nnotifydevicesDS.foreach(d => logAlerts(Console.err, d, \"ACTION NEED! HIGH TEPERATURE AND C02 LEVLES\", \"kafka\"))"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.types._\n\n// a bit longish, nested, and convuloted JSON schema :)\nval nestSchema2 = new StructType()\n      .add(\"devices\", \n        new StructType()\n          .add(\"thermostats\", MapType(StringType,\n            new StructType()\n              .add(\"device_id\", StringType)\n              .add(\"locale\", StringType)\n              .add(\"software_version\", StringType)\n              .add(\"structure_id\", StringType)\n              .add(\"where_name\", StringType)\n              .add(\"last_connection\", StringType)\n              .add(\"is_online\", BooleanType)\n              .add(\"can_cool\", BooleanType)\n              .add(\"can_heat\", BooleanType)\n              .add(\"is_using_emergency_heat\", BooleanType)\n              .add(\"has_fan\", BooleanType)\n              .add(\"fan_timer_active\", BooleanType)\n              .add(\"fan_timer_timeout\", StringType)\n              .add(\"temperature_scale\", StringType)\n              .add(\"target_temperature_f\", DoubleType)\n              .add(\"target_temperature_high_f\", DoubleType)\n              .add(\"target_temperature_low_f\", DoubleType)\n              .add(\"eco_temperature_high_f\", DoubleType)\n              .add(\"eco_temperature_low_f\", DoubleType)\n              .add(\"away_temperature_high_f\", DoubleType)\n              .add(\"away_temperature_low_f\", DoubleType)\n              .add(\"hvac_mode\", StringType)\n              .add(\"humidity\", LongType)\n              .add(\"hvac_state\", StringType)\n              .add(\"is_locked\", StringType)\n              .add(\"locked_temp_min_f\", DoubleType)\n              .add(\"locked_temp_max_f\", DoubleType)))\n           .add(\"smoke_co_alarms\", MapType(StringType,\n             new StructType()\n             .add(\"device_id\", StringType)\n             .add(\"locale\", StringType)\n             .add(\"software_version\", StringType)\n             .add(\"structure_id\", StringType)\n             .add(\"where_name\", StringType)\n             .add(\"last_connection\", StringType)\n             .add(\"is_online\", BooleanType)\n             .add(\"battery_health\", StringType)\n             .add(\"co_alarm_state\", StringType)\n             .add(\"smoke_alarm_state\", StringType)\n             .add(\"is_manual_test_active\", BooleanType)\n             .add(\"last_manual_test_time\", StringType)\n             .add(\"ui_color_state\", StringType)))\n           .add(\"cameras\", MapType(StringType, \n               new StructType()\n                .add(\"device_id\", StringType)\n                .add(\"software_version\", StringType)\n                .add(\"structure_id\", StringType)\n                .add(\"where_name\", StringType)\n                .add(\"is_online\", BooleanType)\n                .add(\"is_streaming\", BooleanType)\n                .add(\"is_audio_input_enabled\", BooleanType)\n                .add(\"last_is_online_change\", StringType)\n                .add(\"is_video_history_enabled\", BooleanType)\n                .add(\"web_url\", StringType)\n                .add(\"app_url\", StringType)\n                .add(\"is_public_share_enabled\", BooleanType)\n                .add(\"activity_zones\",\n                  new StructType()\n                    .add(\"name\", StringType)\n                    .add(\"id\", LongType))\n                .add(\"last_event\", StringType))))"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["%scala\nval nestDataDS2 = Seq(\"\"\"{\n    \"devices\": {\n       \"thermostats\": {\n          \"peyiJNo0IldT2YlIVtYaGQ\": {\n            \"device_id\": \"peyiJNo0IldT2YlIVtYaGQ\",\n            \"locale\": \"en-US\",\n            \"software_version\": \"4.0\",\n            \"structure_id\": \"VqFabWH21nwVyd4RWgJgNb292wa7hG_dUwo2i2SG7j3-BOLY0BA4sw\",\n            \"where_name\": \"Hallway Upstairs\",\n            \"last_connection\": \"2016-10-31T23:59:59.000Z\",\n            \"is_online\": true,\n            \"can_cool\": true,\n            \"can_heat\": true,\n            \"is_using_emergency_heat\": true,\n            \"has_fan\": true,\n            \"fan_timer_active\": true,\n            \"fan_timer_timeout\": \"2016-10-31T23:59:59.000Z\",\n            \"temperature_scale\": \"F\",\n            \"target_temperature_f\": 72,\n            \"target_temperature_high_f\": 80,\n            \"target_temperature_low_f\": 65,\n            \"eco_temperature_high_f\": 80,\n            \"eco_temperature_low_f\": 65,\n            \"away_temperature_high_f\": 80,\n            \"away_temperature_low_f\": 65,\n            \"hvac_mode\": \"heat\",\n            \"humidity\": 40,\n            \"hvac_state\": \"heating\",\n            \"is_locked\": true,\n            \"locked_temp_min_f\": 65,\n            \"locked_temp_max_f\": 80\n            }\n          },\n          \"smoke_co_alarms\": {\n            \"RTMTKxsQTCxzVcsySOHPxKoF4OyCifrs\": {\n              \"device_id\": \"RTMTKxsQTCxzVcsySOHPxKoF4OyCifrs\",\n              \"locale\": \"en-US\",\n              \"software_version\": \"1.01\",\n              \"structure_id\": \"VqFabWH21nwVyd4RWgJgNb292wa7hG_dUwo2i2SG7j3-BOLY0BA4sw\",\n              \"where_name\": \"Jane's Room\",\n              \"last_connection\": \"2016-10-31T23:59:59.000Z\",\n              \"is_online\": true,\n              \"battery_health\": \"ok\",\n              \"co_alarm_state\": \"ok\",\n              \"smoke_alarm_state\": \"ok\",\n              \"is_manual_test_active\": true,\n              \"last_manual_test_time\": \"2016-10-31T23:59:59.000Z\",\n              \"ui_color_state\": \"gray\"\n              }\n            },\n         \"cameras\": {\n          \"awJo6rH0IldT2YlIVtYaGQ\": {\n            \"device_id\": \"awJo6rH\",\n            \"software_version\": \"4.0\",\n            \"structure_id\": \"VqFabWH21nwVyd4RWgJgNb292wa7hG_dUwo2i2SG7j3-BOLY0BA4sw\",\n            \"where_name\": \"Foyer\",\n            \"is_online\": true,\n            \"is_streaming\": true,\n            \"is_audio_input_enabled\": true,\n            \"last_is_online_change\": \"2016-12-29T18:42:00.000Z\",\n            \"is_video_history_enabled\": true,\n            \"web_url\": \"https://home.nest.com/cameras/device_id?auth=access_token\",\n            \"app_url\": \"nestmobile://cameras/device_id?auth=access_token\",\n            \"is_public_share_enabled\": true,\n            \"activity_zones\": { \"name\": \"Walkway\", \"id\": 244083 },\n            \"last_event\": \"2016-10-31T23:59:59.000Z\"\n            }\n          }\n        }\n       }\"\"\").toDS"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["%scala\nval nestDF2 = spark                            // spark session \n            .read                             //  get DataFrameReader\n            .schema(nestSchema2)             //  use the defined schema above and read format as JSON\n            .json(nestDataDS2.rdd)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["%scala\ndisplay(nestDF2)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["%scala\nval stringJsonDF = nestDF2.select(to_json(struct($\"*\"))).toDF(\"nestDevice\")"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["%scala\nstringJsonDF.printSchema"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["%scala\ndisplay(stringJsonDF)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["%scala\nval mapColumnsDF = nestDF2.select($\"devices\".getItem(\"smoke_co_alarms\").alias (\"smoke_alarms\"),\n                                  $\"devices\".getItem(\"cameras\").alias (\"cameras\"),\n                                  $\"devices\".getItem(\"thermostats\").alias (\"thermostats\"))"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["%scala\ndisplay(mapColumnsDF)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["%scala\nval explodedThermostatsDF = mapColumnsDF.select(explode($\"thermostats\"))\nval explodedCamerasDF = mapColumnsDF.select(explode($\"cameras\"))\n//or you could use the original nestDF2 and use the devices.X notation\nval explodedSmokedAlarmsDF =  nestDF2.select(explode($\"devices.smoke_co_alarms\"))"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["%scala\ndisplay(explodedThermostatsDF)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["%scala\nval thermostateDF = explodedThermostatsDF.select($\"value\".getItem(\"device_id\").alias(\"device_id\"), \n                                                 $\"value\".getItem(\"locale\").alias(\"locale\"),\n                                                 $\"value\".getItem(\"where_name\").alias(\"location\"),\n                                                 $\"value\".getItem(\"last_connection\").alias(\"last_connected\"),\n                                                 $\"value\".getItem(\"humidity\").alias(\"humidity\"),\n                                                 $\"value\".getItem(\"target_temperature_f\").alias(\"target_temperature_f\"),\n                                                 $\"value\".getItem(\"hvac_mode\").alias(\"mode\"),\n                                                 $\"value\".getItem(\"software_version\").alias(\"version\"))\n\nval cameraDF = explodedCamerasDF.select($\"value\".getItem(\"device_id\").alias(\"device_id\"),\n                                        $\"value\".getItem(\"where_name\").alias(\"location\"),\n                                        $\"value\".getItem(\"software_version\").alias(\"version\"),\n                                        $\"value\".getItem(\"activity_zones\").getItem(\"name\").alias(\"name\"),\n                                        $\"value\".getItem(\"activity_zones\").getItem(\"id\").alias(\"id\"))\n                                         \nval smokedAlarmsDF = explodedSmokedAlarmsDF.select($\"value\".getItem(\"device_id\").alias(\"device_id\"),\n                                                   $\"value\".getItem(\"where_name\").alias(\"location\"),\n                                                   $\"value\".getItem(\"software_version\").alias(\"version\"),\n                                                   $\"value\".getItem(\"last_connection\").alias(\"last_connected\"),\n                                                   $\"value\".getItem(\"battery_health\").alias(\"battery_health\"))"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["%scala\ndisplay(thermostateDF)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["%scala\ndisplay(cameraDF)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["%scala\ndisplay(smokedAlarmsDF)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["%scala\nval joineDFs = thermostateDF.join(cameraDF, \"version\")"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["%scala\ndisplay(joineDFs)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["# import pyspark class Row from module sql\nfrom pyspark.sql import *\n\n# Create Example Data - Departments and Employees\n\n# Create the Departments\ndepartment1 = Row(id='123456', name='Computer Science')\ndepartment2 = Row(id='789012', name='Mechanical Engineering')\ndepartment3 = Row(id='345678', name='Theater and Drama')\ndepartment4 = Row(id='901234', name='Indoor Recreation')\n\n# Create the Employees\nEmployee = Row(\"firstName\", \"lastName\", \"email\", \"salary\")\nemployee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000)\nemployee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000)\nemployee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000)\nemployee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000)\n\n# Create the DepartmentWithEmployees instances from Departments and Employees\ndepartmentWithEmployees1 = Row(department=department1, employees=[employee1, employee2])\ndepartmentWithEmployees2 = Row(department=department2, employees=[employee3, employee4])\ndepartmentWithEmployees3 = Row(department=department3, employees=[employee1, employee4])\ndepartmentWithEmployees4 = Row(department=department4, employees=[employee2, employee3])\n\nprint department1\nprint employee2\nprint departmentWithEmployees1.employees[0].email"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["departmentsWithEmployeesSeq1 = [departmentWithEmployees1, departmentWithEmployees2]\ndf1 = sqlContext.createDataFrame(departmentsWithEmployeesSeq1)\n\ndisplay(df1)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["departmentsWithEmployeesSeq2 = [departmentWithEmployees3, departmentWithEmployees4]\ndf2 = sqlContext.createDataFrame(departmentsWithEmployeesSeq2)\n\ndisplay(df2)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["unionDF = df1.unionAll(df2)\ndisplay(unionDF)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["# Remove the file if it exists\ndbutils.fs.rm(\"/tmp/databricks-df-example.parquet\", True)\nunionDF.write.parquet(\"/tmp/databricks-df-example.parquet\")"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["parquetDF = sqlContext.read.parquet(\"/tmp/databricks-df-example.parquet\")\ndisplay(parquetDF)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["from pyspark.sql.functions import explode\n\ndf = parquetDF.select(explode(\"employees\").alias(\"e\"))\nexplodeDF = df.selectExpr(\"e.firstName\", \"e.lastName\", \"e.email\", \"e.salary\")\n\ndisplay(explodeDF)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["filterDF = explodeDF.filter(explodeDF.firstName == \"xiangrui\").sort(explodeDF.lastName)\ndisplay(filterDF)"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["from pyspark.sql.functions import col, asc\n\n# Use `|` instead of `or`\nfilterDF = explodeDF.filter((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\ndisplay(filterDF)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["whereDF = explodeDF.where((col(\"firstName\") == \"xiangrui\") | (col(\"firstName\") == \"michael\")).sort(asc(\"lastName\"))\ndisplay(whereDF)"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["nonNullDF = explodeDF.fillna(\"--\")\ndisplay(nonNullDF)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["filterNonNullDF = explodeDF.filter(col(\"firstName\").isNull() | col(\"lastName\").isNull()).sort(\"email\")\ndisplay(filterNonNullDF)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct\n\ncountDistinctDF = explodeDF.select(\"firstName\", \"lastName\")\\\n  .groupBy(\"firstName\", \"lastName\")\\\n  .agg(countDistinct(\"firstName\"))\n\ndisplay(countDistinctDF)"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["countDistinctDF.explain()"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["# register the DataFrame as a temp table so that we can query it using SQL\nexplodeDF.registerTempTable(\"databricks_df_example\")\n\n# Perform the same query as the DataFrame above and return ``explain``\ncountDistinctDF_sql = sqlContext.sql(\"SELECT firstName, lastName, count(distinct firstName) as distinct_first_names FROM databricks_df_example GROUP BY firstName, lastName\")\n\ncountDistinctDF_sql.explain()"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["salarySumDF = explodeDF.agg({\"salary\" : \"sum\"})\ndisplay(salarySumDF)"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["type(explodeDF.salary)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["explodeDF.describe(\"salary\").show()"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["import pandas as pd\nimport matplotlib.pyplot as plt\nplt.clf()\npdDF = nonNullDF.toPandas()\npdDF.plot(x='firstName', y='salary', kind='bar', rot=45)\ndisplay()"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["dbutils.fs.rm(\"/tmp/databricks-df-example.parquet\", True)"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["from pyspark.sql import functions as F\nfrom pyspark.sql.types import *\n\n# Build an example DataFrame dataset to work with.\ndbutils.fs.rm(\"/tmp/dataframe_sample.csv\", True)\ndbutils.fs.put(\"/tmp/dataframe_sample.csv\", \"\"\"id|end_date|start_date|location\n1|2015-10-14 00:00:00|2015-09-14 00:00:00|CA-SF\n2|2015-10-15 01:00:20|2015-08-14 00:00:00|CA-SD\n3|2015-10-16 02:30:00|2015-01-14 00:00:00|NY-NY\n4|2015-10-17 03:00:20|2015-02-14 00:00:00|NY-NY\n5|2015-10-18 04:30:00|2014-04-14 00:00:00|CA-SD\n\"\"\", True)\n\nformatPackage = \"csv\" if sc.version > '1.6' else \"com.databricks.spark.csv\"\ndf = sqlContext.read.format(formatPackage).options(header='true', delimiter = '|').load(\"/tmp/dataframe_sample.csv\")\ndf.printSchema()"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["# Instead of registering a UDF, call the builtin functions to perform operations on the columns.\n# This will provide a performance improvement as the builtins compile and run in the platform's JVM.\n\n# Convert to a Date type\ndf = df.withColumn('date', F.to_date(df.end_date))\n\n# Parse out the date only\ndf = df.withColumn('date_only', F.regexp_replace(df.end_date,' (\\d+)[:](\\d+)[:](\\d+).*$', ''))\n\n# Split a string and index a field\ndf = df.withColumn('city', F.split(df.location, '-')[1])\n\n# Perform a date diff function\ndf = df.withColumn('date_diff', F.datediff(F.to_date(df.end_date), F.to_date(df.start_date)))"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["df.registerTempTable(\"sample_df\")\ndisplay(sql(\"select * from sample_df\"))"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["rdd_json = df.toJSON()\nrdd_json.take(2)"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["from pyspark.sql import functions as F\n\nadd_n = udf(lambda x, y: x + y, IntegerType())\n\n# We register a UDF that adds a column to the DataFrame, and we cast the id column to an Integer type.\ndf = df.withColumn('id_offset', add_n(F.lit(1000), df.id.cast(IntegerType())))"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["display(df)"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["# any constants used by UDF will automatically pass through to workers\nN = 90\nlast_n_days = udf(lambda x: x < N, BooleanType())\n\ndf_filtered = df.filter(last_n_days(df.date_diff))\ndisplay(df_filtered)"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["df_1 = table(\"sample_df\")\ndf_2 = sqlContext.sql(\"select * from sample_df\")"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["sqlContext.clearCache()\nsqlContext.cacheTable(\"sample_df\")\nsqlContext.uncacheTable(\"sample_df\")"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["agg_df = df.groupBy(\"location\").agg(F.min(\"id\"), F.count(\"id\"), F.avg(\"date_diff\"))\ndisplay(agg_df)"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["df = df.withColumn('end_month', F.month('end_date'))\ndf = df.withColumn('end_year', F.year('end_date'))\ndf.write.partitionBy(\"end_year\", \"end_month\").parquet(\"/tmp/sample_table\")\ndisplay(dbutils.fs.ls(\"/tmp/sample_table\"))"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":["null_item_schema = StructType([StructField(\"col1\", StringType(), True),\n                               StructField(\"col2\", IntegerType(), True)])\nnull_df = sqlContext.createDataFrame([(\"test\", 1), (None, 2)], null_item_schema)\ndisplay(null_df.filter(\"col1 IS NOT NULL\"))"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["adult_df = sqlContext.read.\\\n    format(\"com.databricks.spark.csv\").\\\n    option(\"header\", \"false\").\\\n    option(\"inferSchema\", \"true\").load(\"dbfs:/databricks-datasets/adult/adult.data\")\nadult_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["%scala\n// Create the case classes for our domain\ncase class Department(id: String, name: String)\ncase class Employee(firstName: String, lastName: String, email: String, salary: Int)\ncase class DepartmentWithEmployees(department: Department, employees: Seq[Employee])\n\n// Create the Departments\nval department1 = new Department(\"123456\", \"Computer Science\")\nval department2 = new Department(\"789012\", \"Mechanical Engineering\")\nval department3 = new Department(\"345678\", \"Theater and Drama\")\nval department4 = new Department(\"901234\", \"Indoor Recreation\")\n\n// Create the Employees\nval employee1 = new Employee(\"michael\", \"armbrust\", \"no-reply@berkeley.edu\", 100000)\nval employee2 = new Employee(\"xiangrui\", \"meng\", \"no-reply@stanford.edu\", 120000)\nval employee3 = new Employee(\"matei\", null, \"no-reply@waterloo.edu\", 140000)\nval employee4 = new Employee(null, \"wendell\", \"no-reply@princeton.edu\", 160000)\n\n// Create the DepartmentWithEmployees instances from Departments and Employees\nval departmentWithEmployees1 = new DepartmentWithEmployees(department1, Seq(employee1, employee2))\nval departmentWithEmployees2 = new DepartmentWithEmployees(department2, Seq(employee3, employee4))\nval departmentWithEmployees3 = new DepartmentWithEmployees(department3, Seq(employee1, employee4))\nval departmentWithEmployees4 = new DepartmentWithEmployees(department4, Seq(employee2, employee3))"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["%scala\nval departmentsWithEmployeesSeq1 = Seq(departmentWithEmployees1, departmentWithEmployees2)\nval df1 = departmentsWithEmployeesSeq1.toDF()\ndisplay(df1)"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["%scala\nval departmentsWithEmployeesSeq2 = Seq(departmentWithEmployees3, departmentWithEmployees4)\nval df2 = departmentsWithEmployeesSeq2.toDF()\ndisplay(df2)"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["%scala\nval unionDF = df1.unionAll(df2)\ndisplay(unionDF)"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["%scala\n// Remove the file if it exists\ndbutils.fs.rm(\"/tmp/databricks-df-example.parquet\", true)\nunionDF.write.parquet(\"/tmp/databricks-df-example.parquet\")"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["%scala\nval parquetDF = sqlContext.read.parquet(\"/tmp/databricks-df-example.parquet\")"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["%scala\nval explodeDF = parquetDF.explode($\"employees\") {\n  case Row(employee: Seq[Row]) => employee.map{ employee =>\n    val firstName = employee(0).asInstanceOf[String]\n    val lastName = employee(1).asInstanceOf[String]\n    val email = employee(2).asInstanceOf[String]\n    val salary = employee(3).asInstanceOf[Int]\n    Employee(firstName, lastName, email, salary)\n  }\n}.cache()\ndisplay(explodeDF)"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"code","source":["%scala\nexplodeDF"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["%scala\nval filterDF = explodeDF\n  .filter($\"firstName\" === \"xiangrui\" || $\"firstName\" === \"michael\")\n  .sort($\"lastName\".asc)\ndisplay(filterDF)"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["%scala\nval whereDF = explodeDF.where(($\"firstName\" === \"xiangrui\") || ($\"firstName\" === \"michael\")).sort($\"lastName\".asc)\ndisplay(whereDF)"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["%scala\nval naFunctions = explodeDF.na\nval nonNullDF = naFunctions.fill(\"--\")\ndisplay(nonNullDF)"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"code","source":["%scala\nval filterNonNullDF = nonNullDF.filter($\"firstName\" === \"\" || $\"lastName\" === \"\").sort($\"email\".asc)\ndisplay(filterNonNullDF)"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions._\nval countDistinctDF = nonNullDF.select($\"firstName\", $\"lastName\")\n  .groupBy($\"firstName\", $\"lastName\")\n  .agg(countDistinct($\"firstName\") as \"distinct_first_names\")\ndisplay(countDistinctDF)"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["%scala\ncountDistinctDF.explain()"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["%scala\n// register the DataFrame as a temp table so that we can query it using SQL\nnonNullDF.registerTempTable(\"databricks_df_example\")\n\n// Perform the same query as the DataFrame above and return ``explain``\nsqlContext.sql(\"\"\"\nSELECT firstName, lastName, count(distinct firstName) as distinct_first_names\nFROM databricks_df_example\nGROUP BY firstName, lastName\n\"\"\").explain"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"code","source":["%scala\n// Sum up all the salaries\nval salarySumDF = nonNullDF.agg(\"salary\" -> \"sum\")\ndisplay(salarySumDF)"],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"code","source":["%scala\nnonNullDF.describe(\"salary\").show()"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["%scala\nval veryNestedDF = Seq((\"1\", (2, (3, 4)))).toDF()"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\n\nimplicit class DataFrameFlattener(df: DataFrame) {\n  def flattenSchema: DataFrame = {\n    df.select(flatten(Nil, df.schema): _*)\n  }\n\n  protected def flatten(path: Seq[String], schema: DataType): Seq[Column] = schema match {\n    case s: StructType => s.fields.flatMap(f => flatten(path :+ f.name, f.dataType))\n    case other => col(path.map(n => s\"`$n`\").mkString(\".\")).as(path.mkString(\".\")) :: Nil\n  }\n}"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"code","source":["%scala\ndisplay(veryNestedDF)"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"code","source":["%scala\ndisplay(veryNestedDF.flattenSchema)"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"code","source":["%scala\ndbutils.fs.rm(\"/tmp/databricks-df-example.parquet\", true)"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\nimport org.apache.hadoop.io.LongWritable\nimport org.apache.hadoop.io.Text\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat\n\n// Build an example DataFrame dataset to work with.\ndbutils.fs.rm(\"/tmp/dataframe_sample.csv\", true)\ndbutils.fs.put(\"/tmp/dataframe_sample.csv\", \"\"\"\nid|end_date|start_date|location\n1|2015-10-14 00:00:00|2015-09-14 00:00:00|CA-SF\n2|2015-10-15 01:00:20|2015-08-14 00:00:00|CA-SD\n3|2015-10-16 02:30:00|2015-01-14 00:00:00|NY-NY\n4|2015-10-17 03:00:20|2015-02-14 00:00:00|NY-NY\n5|2015-10-18 04:30:00|2014-04-14 00:00:00|CA-LA\n\"\"\", true)\n\nval conf = new Configuration\nconf.set(\"textinputformat.record.delimiter\", \"\\n\")\nval rdd = sc.newAPIHadoopFile(\"/tmp/dataframe_sample.csv\", classOf[TextInputFormat], classOf[LongWritable], classOf[Text], conf).map(_._2.toString).filter(_.nonEmpty)\n\nval header = rdd.first()\n// Parse the header line\nval rdd_noheader = rdd.filter(x => !x.contains(\"id\"))\n// Convert the RDD[String] to an RDD[Rows]. Create an array using the delimiter and use Row.fromSeq()\nval row_rdd = rdd_noheader.map(x => x.split('|')).map(x => Row.fromSeq(x))\n\nval df_schema =\n  StructType(\n    header.split('|').map(fieldName => StructField(fieldName, StringType, true)))\n\nvar df = sqlContext.createDataFrame(row_rdd, df_schema)\ndf.printSchema"],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"code","source":["%scala\n// Instead of registering a UDF, call the builtin functions to perform operations on the columns.\n// This will provide a performance improvement as the builtins compile and run in the platform's JVM.\n\n// Convert to a Date type\nval timestamp2datetype: (Column) => Column = (x) => { to_date(x) }\ndf = df.withColumn(\"date\", timestamp2datetype(col(\"end_date\")))\n\n// Parse out the date only\nval timestamp2date: (Column) => Column = (x) => { regexp_replace(x,\" (\\\\d+)[:](\\\\d+)[:](\\\\d+).*$\", \"\") }\ndf = df.withColumn(\"date_only\", timestamp2date(col(\"end_date\")))\n\n// Split a string and index a field\nval parse_city: (Column) => Column = (x) => { split(x, \"-\")(1) }\ndf = df.withColumn(\"city\", parse_city(col(\"location\")))\n\n// Perform a date diff function\nval dateDiff: (Column, Column) => Column = (x, y) => { datediff(to_date(y), to_date(x)) }\ndf = df.withColumn(\"date_diff\", dateDiff(col(\"start_date\"), col(\"end_date\")))"],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"code","source":["%scala\ndf.registerTempTable(\"sample_df\")\ndisplay(sql(\"select * from sample_df\"))"],"metadata":{},"outputs":[],"execution_count":113},{"cell_type":"code","source":["%scala\nval rdd_json = df.toJSON\nrdd_json.take(2).foreach(println)"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"code","source":["%scala\nval add_n = udf((x: Integer, y: Integer) => x + y)\n\n// We register a UDF that adds a column to the DataFrame, and we cast the id column to an Integer type.\ndf = df.withColumn(\"id_offset\", add_n(lit(1000), col(\"id\").cast(\"int\")))\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":115},{"cell_type":"code","source":["%scala\nval last_n_days = udf((x: Integer, y: Integer) => {\n  if (x < y) true else false\n})\n\n//last_n_days = udf(lambda x, y: True if x < y else False, BooleanType())\n\nval df_filtered = df.filter(last_n_days(col(\"date_diff\"), lit(90)))\ndisplay(df_filtered)"],"metadata":{},"outputs":[],"execution_count":116},{"cell_type":"code","source":["%scala\n// Both return DataFrame types\nval df_1 = table(\"sample_df\")\nval df_2 = sqlContext.sql(\"select * from sample_df\")"],"metadata":{},"outputs":[],"execution_count":117},{"cell_type":"code","source":["%scala\nsqlContext.clearCache()\nsqlContext.cacheTable(\"sample_df\")\nsqlContext.uncacheTable(\"sample_df\")"],"metadata":{},"outputs":[],"execution_count":118},{"cell_type":"code","source":["%scala\n// Provide the min, count, and avg and groupBy the location column. Diplay the results\nvar agg_df = df.groupBy(\"location\").agg(min(\"id\"), count(\"id\"), avg(\"date_diff\"))\ndisplay(agg_df)"],"metadata":{},"outputs":[],"execution_count":119},{"cell_type":"code","source":["%scala\ndf = df.withColumn(\"end_month\", month(col(\"end_date\")))\ndf = df.withColumn(\"end_year\", year(col(\"end_date\")))\ndbutils.fs.rm(\"/tmp/sample_table\", true)\ndf.write.partitionBy(\"end_year\", \"end_month\").parquet(\"/tmp/sample_table\")\ndisplay(dbutils.fs.ls(\"/tmp/sample_table\"))"],"metadata":{},"outputs":[],"execution_count":120},{"cell_type":"code","source":["%scala\nval null_item_schema = StructType(Array(StructField(\"col1\", StringType, true),\n                               StructField(\"col2\", IntegerType, true)))\n\nval null_dataset = sc.parallelize(Array((\"test\", 1 ), (null, 2))).map(x => Row.fromTuple(x))\nval null_df = sqlContext.createDataFrame(null_dataset, null_item_schema)\ndisplay(null_df.filter(\"col1 IS NOT NULL\"))"],"metadata":{},"outputs":[],"execution_count":121},{"cell_type":"code","source":["%scala\nval adult_df = sqlContext.read.\n    format(\"com.databricks.spark.csv\").\n    option(\"header\", \"false\").\n    option(\"inferSchema\", \"true\").load(\"dbfs:/databricks-datasets/adult/adult.data\")\nadult_df.printSchema()"],"metadata":{},"outputs":[],"execution_count":122},{"cell_type":"code","source":["%scala\nrequire(sc.version.replace(\".\", \"\").substring(0,3).toInt >= 160, \"Spark 1.6.0+ required to run this notebook.\")"],"metadata":{},"outputs":[],"execution_count":123},{"cell_type":"code","source":["%scala\nval dataset = Seq(1, 2, 3).toDS()\ndataset.show()"],"metadata":{},"outputs":[],"execution_count":124},{"cell_type":"code","source":["%scala\ncase class Person(name: String, age: Int)\n\nval personDS = Seq(Person(\"Max\", 33), Person(\"Adam\", 32), Person(\"Muller\", 62)).toDS()\npersonDS.show()"],"metadata":{},"outputs":[],"execution_count":125},{"cell_type":"code","source":["%scala\nval rdd = sc.parallelize(Seq((1, \"Spark\"), (2, \"Databricks\")))\nval integerDS = rdd.toDS()\nintegerDS.show()"],"metadata":{},"outputs":[],"execution_count":126},{"cell_type":"code","source":["%scala\ncase class Company(name: String, foundingYear: Int, numEmployees: Int)\nval inputSeq = Seq(Company(\"ABC\", 1998, 310), Company(\"XYZ\", 1983, 904), Company(\"NOP\", 2005, 83))\nval df = sc.parallelize(inputSeq).toDF()\n\nval companyDS = df.as[Company]\ncompanyDS.show()"],"metadata":{},"outputs":[],"execution_count":127},{"cell_type":"code","source":["%scala\nval rdd = sc.parallelize(Seq((1, \"Spark\"), (2, \"Databricks\"), (3, \"Notebook\")))\nval df = rdd.toDF(\"Id\", \"Name\")\n\nval dataset = df.as[(Int, String)]\ndataset.show()"],"metadata":{},"outputs":[],"execution_count":128},{"cell_type":"code","source":["%scala\nval wordsDataset = sc.parallelize(Seq(\"Spark I am your father\", \"May the spark be with you\", \"Spark I am your father\")).toDS()\nval groupedDataset = wordsDataset.flatMap(_.toLowerCase.split(\" \"))\n                                 .filter(_ != \"\")\n                                 .groupBy(\"value\")\nval countsDataset = groupedDataset.count()\ncountsDataset.show()"],"metadata":{},"outputs":[],"execution_count":129},{"cell_type":"code","source":["%scala\ncase class Employee(name: String, age: Int, departmentId: Int, salary: Double)\ncase class Department(id: Int, name: String)\n\ncase class Record(name: String, age: Int, salary: Double, departmentId: Int, departmentName: String)\ncase class ResultSet(departmentId: Int, departmentName: String, avgSalary: Double)\n\nval employeeDataSet1 = sc.parallelize(Seq(Employee(\"Max\", 22, 1, 100000.0), Employee(\"Adam\", 33, 2, 93000.0), Employee(\"Eve\", 35, 2, 89999.0), Employee(\"Muller\", 39, 3, 120000.0))).toDS()\nval employeeDataSet2 = sc.parallelize(Seq(Employee(\"John\", 26, 1, 990000.0), Employee(\"Joe\", 38, 3, 115000.0))).toDS()\nval departmentDataSet = sc.parallelize(Seq(Department(1, \"Engineering\"), Department(2, \"Marketing\"), Department(3, \"Sales\"))).toDS()\n\nval employeeDataset = employeeDataSet1.union(employeeDataSet2)\n\ndef averageSalary(key: (Int, String), iterator: Iterator[Record]): ResultSet = {\n   val (total, count) = iterator.foldLeft(0.0, 0.0) {\n       case ((total, count), x) => (total + x.salary, count + 1)\n   }\n   ResultSet(key._1, key._2, total/count)\n}\n\nval averageSalaryDataset = employeeDataset.joinWith(departmentDataSet, $\"departmentId\" === $\"id\", \"inner\")\n                                           .map(record => Record(record._1.name, record._1.age, record._1.salary, record._1.departmentId, record._2.name))\n                                           .filter(record => record.age > 25)\n                                           .groupBy($\"departmentId\", $\"departmentName\")\n                                           .avg()"],"metadata":{},"outputs":[],"execution_count":130},{"cell_type":"code","source":["%scala\naverageSalaryDataset.show()"],"metadata":{},"outputs":[],"execution_count":131},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions._\n\nval wordsDataset = sc.parallelize(Seq(\"Spark I am your father\", \"May the spark be with you\", \"Spark I am your father\")).toDS()\nval result = wordsDataset\n               .flatMap(_.split(\" \"))               // Split on whitespace\n               .filter(_ != \"\")                     // Filter empty words\n               .map(_.toLowerCase())\n               .toDF()                              // Convert to DataFrame to perform aggregation / sorting\n               .groupBy($\"value\")                   // Count number of occurences of each word\n               .agg(count(\"*\") as \"numOccurances\")\n               .orderBy($\"numOccurances\" desc)      // Show most common words first\nresult.show()"],"metadata":{},"outputs":[],"execution_count":132},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":133}],"metadata":{"name":"databricks docs 7 (from dataframes and datasets","notebookId":3117922773650607},"nbformat":4,"nbformat_minor":0}
