{"cells":[{"cell_type":"code","source":["%sh\nsbt assembly\n"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%sh\ncurl -n \\\n-F filedata=@\"target/scala-2.10/ScalaSparkTemplate-assembly-0.2-SNAPSHOT.jar\" \\\n-F path=\"/docs/tutorial.jar\" \\\n-F overwrite=true \\\nhttps://YOURACCOUNT.cloud.databricks.com/api/2.0/dbfs/put"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%sh\ncurl -n \\\nhttps://YOURACCOUNT.cloud.databricks.com/api/2.0/clusters/spark-versions"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%sh\ncurl -n \\\n-X POST -H 'Content-Type: application/json' \\\n-d '{\n      \"name\": \"my example job\",\n      \"new_cluster\": {\n        \"spark_version\": \"2.0.1-db1-scala2.10\",\n        \"node_type_id\": \"r3.xlarge\",\n        \"aws_attributes\": {\"availability\": \"ON_DEMAND\"},\n        \"num_workers\": 1\n        },\n     \"libraries\": [{\"jar\": \"dbfs:/docs/tutorial.jar\"}],\n     \"spark_jar_task\": {\n        \"main_class_name\":\"databricks.examples.ExampleClass\",\n        \"parameters\": \"hello there\"\n        }\n}' \\\nhttps://YOURACCOUNT.cloud.databricks.com/api/2.0/jobs/create"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sh\ncurl -n \\\n-X POST -H 'Content-Type: application/json' \\\n-d '{\n      \"job_id\": 84005,\n      \"jar_params\": [\"hello\", \"params\", \"that are overwritten at run time\"]\n}' \\\nhttps://YOURACCOUNT.cloud.databricks.com/api/2.0/jobs/run-now"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%sh\ncurl -n https://YOURACCOUNT.cloud.databricks.com/api/2.0/jobs/runs/get?run_id=786200"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%sh\nimport json\nimport base64\nimport urllib\n\nACCOUNT = 'YOURACCOUNT'\nUSER = 'USERNAME'\nPASSWORD = 'PASSWORD'\nBASE_URL = 'https://%s:%s@%s.cloud.databricks.com/api/2.0/dbfs/' %\\\n    (urllib.quote(USER), urllib.quote(PASSWORD), ACCOUNT)\n\ndef dbfs_rpc(action, body):\n    \"\"\" A helper function to make the DBFS API request, request/response is encoded/decoded as JSON \"\"\"\n    res = urllib.urlopen(BASE_URL + action, json.dumps(body)).read()\n    return json.loads(res)\n\n# Create a handle which will be used to add blocks\nhandle = dbfs_rpc(\"create\", {\"path\": \"/temp/upload_large_file\", \"overwrite\": \"true\"})['handle']\nwith open('/a/local/file') as f:\n    while True:\n        # A block can be at most 1MB\n        block = f.read(1 << 20)\n        if not block:\n            break\n        data = base64.standard_b64encode(block)\n        dbfs_rpc(\"add-block\", {\"handle\": handle, \"data\": data})\n# close the handle to finish uploading\ndbfs_rpc(\"close\", {\"handle\": handle})"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql\nALTER (DATABASE|SCHEMA) db_name SET DBPROPERTIES (key1=val1, ...)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%sql\nCREATE TABLE boxes (width INT, length INT, height INT) USING CSV\n\nCREATE TEMPORARY TABLE boxes\n    (width INT, length INT, height INT)\n    USING PARQUET\n    OPTIONS ('compression'='snappy')\n\nCREATE TABLE rectangles\n    USING PARQUET\n    PARTITIONED BY (width)\n    CLUSTERED BY (length) INTO 8 buckets\n    AS SELECT * FROM boxes\n\nCREATE OR REPLACE TEMPORARY VIEW temp_rectangles\n    AS SELECT * FROM boxes"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":10}],"metadata":{"name":"databricks docs 5 (admin to","notebookId":4289818323228067},"nbformat":4,"nbformat_minor":0}
