{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# Convenience function for turning JSON strings into DataFrames.\ndef jsonToDataFrame(json, schema=None):\n  # SparkSessions are available with Spark 2.0+\n  reader = spark.read\n  if schema:\n    reader.schema(schema)\n  return reader.json(sc.parallelize([json]))"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["schema = StructType().add(\"a\", StructType().add(\"b\", IntegerType()))\n                          \nevents = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n     \"b\": 1\n  }\n}\n\"\"\", schema)\n\ndisplay(events.select(\"a.b\"))"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Using a map\nschema = StructType().add(\"a\", MapType(StringType(), IntegerType()))\n                          \nevents = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n     \"b\": 1\n  }\n}\n\"\"\", schema)\n\ndisplay(events.select(\"a.b\"))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["events = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n     \"b\": 1,\n     \"c\": 2\n  }\n}\n\"\"\")\n\ndisplay(events.select(\"a.*\"))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["events = jsonToDataFrame(\"\"\"\n{\n  \"a\": 1,\n  \"b\": 2,\n  \"c\": 3\n}\n\"\"\")\n\ndisplay(events.select(struct(col(\"a\").alias(\"y\")).alias(\"x\")))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["events = jsonToDataFrame(\"\"\"\n{\n  \"a\": 1,\n  \"b\": 2\n}\n\"\"\")\n\ndisplay(events.select(struct(\"*\").alias(\"x\")))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["events = jsonToDataFrame(\"\"\"\n{\n  \"a\": [1, 2]\n}\n\"\"\")\n\ndisplay(events.select(col(\"a\").getItem(0).alias(\"x\")))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Using a map\nschema = StructType().add(\"a\", MapType(StringType(), IntegerType()))\n\nevents = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n    \"b\": 1\n  }\n}\n\"\"\", schema)\n\ndisplay(events.select(col(\"a\").getItem(\"b\").alias(\"x\")))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["events = jsonToDataFrame(\"\"\"\n{\n  \"a\": [1, 2]\n}\n\"\"\")\n\ndisplay(events.select(explode(\"a\").alias(\"x\")))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Using a map\nschema = StructType().add(\"a\", MapType(StringType(), IntegerType()))\n\nevents = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n    \"b\": 1,\n    \"c\": 2\n  }\n}\n\"\"\", schema)\n\ndisplay(events.select(explode(\"a\").alias(\"x\", \"y\")))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["events = jsonToDataFrame(\"\"\"\n[{ \"x\": 1 }, { \"x\": 2 }]\n\"\"\")\n\ndisplay(events.select(collect_list(\"x\").alias(\"x\")))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# using an aggregation\nevents = jsonToDataFrame(\"\"\"\n[{ \"x\": 1, \"y\": \"a\" }, { \"x\": 2, \"y\": \"b\" }]\n\"\"\")\n\ndisplay(events.groupBy(\"y\").agg(collect_list(\"x\").alias(\"x\")))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["events = jsonToDataFrame(\"\"\"\n{\n  \"a\": [\n    {\"b\": 1},\n    {\"b\": 2}\n  ]\n}\n\"\"\")\n\ndisplay(events.select(\"a.b\"))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["events = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n    \"b\": 1\n  }\n}\n\"\"\")\n\ndisplay(events.select(to_json(\"a\").alias(\"c\")))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["events = jsonToDataFrame(\"\"\"\n{\n  \"a\": \"{\\\\\"b\\\\\":1}\"\n}\n\"\"\")\n\nschema = StructType().add(\"b\", IntegerType())\ndisplay(events.select(from_json(\"a\", schema).alias(\"c\")))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["events = jsonToDataFrame(\"\"\"\n{\n  \"a\": \"{\\\\\"b\\\\\":{\\\\\"x\\\\\":1,\\\\\"y\\\\\":{\\\\\"z\\\\\":2}}}\"\n}\n\"\"\")\n\nschema = StructType().add(\"b\", StructType().add(\"x\", IntegerType())\n                            .add(\"y\", StringType()))\ndisplay(events.select(from_json(\"a\", schema).alias(\"c\")))\n\n"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["events = jsonToDataFrame(\"\"\"\n{\n  \"a\": \"{\\\\\"b\\\\\":1}\"\n}\n\"\"\")\n\ndisplay(events.select(json_tuple(\"a\", \"b\").alias(\"c\")))\n\n"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["events = jsonToDataFrame(\"\"\"\n[{ \"a\": \"x: 1\" }, { \"a\": \"y: 2\" }]\n\"\"\")\n\ndisplay(events.select(regexp_extract(\"a\", \"([a-z]):\", 1).alias(\"c\")))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\n\n// Convenience function for turning JSON strings into DataFrames.\ndef jsonToDataFrame(json: String, schema: StructType = null): DataFrame = {\n  // SparkSessions are available with Spark 2.0+\n  val reader = spark.read\n  Option(schema).foreach(reader.schema)\n  reader.json(sc.parallelize(Array(json)))\n}"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%scala\n// Using a struct\nval schema = new StructType().add(\"a\", new StructType().add(\"b\", IntegerType))\n                          \nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n     \"b\": 1\n  }\n}\n\"\"\", schema)\n\ndisplay(events.select(\"a.b\"))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%scala\nval schema = new StructType().add(\"a\", MapType(StringType, IntegerType))\n                          \nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n     \"b\": 1\n  }\n}\n\"\"\", schema)\n\ndisplay(events.select(\"a.b\"))"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["%scala\nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n     \"b\": 1,\n     \"c\": 2\n  }\n}\n\"\"\")\n\ndisplay(events.select(\"a.*\"))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%scala\nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": 1,\n  \"b\": 2,\n  \"c\": 3\n}\n\"\"\")\n\ndisplay(events.select(struct('a as 'y) as 'x))\n\n"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["%scala\nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": 1,\n  \"b\": 2\n}\n\"\"\")\n\ndisplay(events.select(struct(\"*\") as 'x))\n\n"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["%scala\nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": [1, 2]\n}\n\"\"\")\n\ndisplay(events.select('a.getItem(0) as 'x))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%scala\nval schema = new StructType().add(\"a\", MapType(StringType, IntegerType))\n\nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n    \"b\": 1\n  }\n}\n\"\"\", schema)\n\ndisplay(events.select('a.getItem(\"b\") as 'x))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%scala\nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": [1, 2]\n}\n\"\"\")\n\ndisplay(events.select(explode('a) as 'x))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["%scala\n// Using a map\nval schema = new StructType().add(\"a\", MapType(StringType, IntegerType))\n\nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n    \"b\": 1,\n    \"c\": 2\n  }\n}\n\"\"\", schema)\n\ndisplay(events.select(explode('a) as (Seq(\"x\", \"y\"))))\n\n"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%scala\nval events = jsonToDataFrame(\"\"\"\n[{ \"x\": 1 }, { \"x\": 2 }]\n\"\"\")\n\ndisplay(events.select(collect_list('x) as 'x))\n\n"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["%scala\n// using an aggregation\nval events = jsonToDataFrame(\"\"\"\n[{ \"x\": 1, \"y\": \"a\" }, { \"x\": 2, \"y\": \"b\" }]\n\"\"\")\n\ndisplay(events.groupBy(\"y\").agg(collect_list('x) as 'x))\n\n\t"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["%scala\nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": [\n    {\"b\": 1},\n    {\"b\": 2}\n  ]\n}\n\"\"\")\n\ndisplay(events.select(\"a.b\"))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["%scala\nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n    \"b\": 1\n  }\n}\n\"\"\")\n\ndisplay(events.select(to_json('a) as 'c))"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["%scala\nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": \"{\\\"b\\\":1}\"\n}\n\"\"\")\n\nval schema = new StructType().add(\"b\", IntegerType)\ndisplay(events.select(from_json('a, schema) as 'c))\n\n"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["%scala\nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": \"{\\\"b\\\":{\\\"x\\\":1,\\\"y\\\":{\\\"z\\\":2}}}\"\n}\n\"\"\")\n\nval schema = new StructType().add(\"b\", new StructType().add(\"x\", IntegerType)\n  .add(\"y\", StringType))\ndisplay(events.select(from_json('a, schema) as 'c))"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["%scala\nval events = jsonToDataFrame(\"\"\"\n{\n  \"a\": \"{\\\"b\\\":1}\"\n}\n\"\"\")\n\ndisplay(events.select(json_tuple('a, \"b\") as 'c))"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["%scala\nval events = jsonToDataFrame(\"\"\"\n[{ \"a\": \"x: 1\" }, { \"a\": \"y: 2\" }]\n\"\"\")\n\ndisplay(events.select(regexp_extract('a, \"([a-z]):\", 1) as 'c))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# Convenience function for turning JSON strings into DataFrames.\ndef jsonToDataFrame(json, schema=None):\n  # SparkSessions are available with Spark 2.0+\n  reader = spark.read\n  if schema:\n    reader.schema(schema)\n  reader.json(sc.parallelize([json])).createOrReplaceTempView(\"events\")"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# Using a struct\nschema = StructType().add(\"a\", StructType().add(\"b\", IntegerType()))\n                          \njsonToDataFrame(\"\"\"\n{\n  \"a\": {\n     \"b\": 1\n  }\n}\n\"\"\", schema)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# Using a map\nschema = StructType().add(\"a\", MapType(StringType(), IntegerType()))\n                          \njsonToDataFrame(\"\"\"\n{\n  \"a\": {\n     \"b\": 1\n  }\n}\n\"\"\", schema)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["%sql\nselect a.b from events"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["jsonToDataFrame(\"\"\"\n{\n  \"a\": {\n     \"b\": 1,\n     \"c\": 2\n  }\n}\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["%sql\nselect a.* from events"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["jsonToDataFrame(\"\"\"\n{\n  \"a\": 1,\n  \"b\": 2,\n  \"c\": 3\n}\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["%sql\nselect named_struct(\"y\", a) as x from events"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["jsonToDataFrame(\"\"\"\n{\n  \"a\": 1,\n  \"b\": 2\n}\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["%sql\nselect struct(*) as x from events"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["jsonToDataFrame(\"\"\"\n{\n  \"a\": [1, 2]\n}\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["%sql\nselect a[0] as x from events"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["# Using a map\nschema = StructType().add(\"a\", MapType(StringType(), IntegerType()))\n\njsonToDataFrame(\"\"\"\n{\n  \"a\": {\n    \"b\": 1\n  }\n}\n\"\"\", schema)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["%sql\nselect a['b'] as x from events"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["jsonToDataFrame(\"\"\"\n{\n  \"a\": [1, 2]\n}\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["%sql\nselect explode(a) as x from events"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["schema = StructType().add(\"a\", MapType(StringType(), IntegerType()))\n\njsonToDataFrame(\"\"\"\n{\n  \"a\": {\n    \"b\": 1,\n    \"c\": 2\n  }\n}\n\"\"\", schema)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["%sql\nselect explode(a) as (x, y) from events"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["jsonToDataFrame(\"\"\"\n[{ \"x\": 1 }, { \"x\": 2 }]\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["%sql\nselect collect_list(x) as x from events"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["jsonToDataFrame(\"\"\"\n[{ \"x\": 1, \"y\": \"a\" }, { \"x\": 2, \"y\": \"b\" }]\n\"\"\")\n"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["%sql\nselect y, collect_list(x) as x from events group by y"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["jsonToDataFrame(\"\"\"\n{\n  \"a\": [\n    {\"b\": 1},\n    {\"b\": 2}\n  ]\n}\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["%sql\nselect a.b from events"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["jsonToDataFrame(\"\"\"\n{\n  \"a\": \"{\\\\\"b\\\\\":1}\"\n}\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["%sql\nselect json_tuple(a, \"b\") as c from events"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["jsonToDataFrame(\"\"\"\n[{ \"a\": \"x: 1\" }, { \"a\": \"y: 2\" }]\n\"\"\")"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["%sql\nselect regexp_extract(a, \"([a-z]):\", 1) as c from events"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW nested_data AS\nSELECT   id AS key,\n         ARRAY(CAST(RAND(1) * 100 AS INT), CAST(RAND(2) * 100 AS INT), CAST(RAND(3) * 100 AS INT), CAST(RAND(4) * 100 AS INT), CAST(RAND(5) * 100 AS INT)) AS values\n         ,\n         ARRAY(ARRAY(CAST(RAND(1) * 100 AS INT), CAST(RAND(2) * 100 AS INT)), ARRAY(CAST(RAND(3) * 100 AS INT), CAST(RAND(4) * 100 AS INT), CAST(RAND(5) * 100 AS INT))) AS nested_values\nFROM range(5)"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["%sql SELECT * FROM nested_data"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["%sql\nSELECT  key,\n        values,\n        TRANSFORM(values, value -> value + 1) AS values_plus_one\nFROM    nested_data"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["%sql\nSELECT  key,\n        values,\n        TRANSFORM(values, value -> value + key) AS values_plus_key\nFROM    nested_data"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["%sql\nSELECT   key,\n         nested_values,\n         TRANSFORM(nested_values,\n           values -> TRANSFORM(values,\n             value -> value + key + SIZE(values))) AS new_nested_values\nFROM     nested_data"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["%sql\nSELECT   key,\n         values,\n         TRANSFORM(values, value -> value + key) transformed_values\nFROM     nested_data"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["%sql\nSELECT   key,\n         values,\n         EXISTS(values, value -> value % 10 == 1) filtered_values\nFROM     nested_data"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["%sql\nSELECT   key,\n         values,\n         FILTER(values, value -> value > 50) filtered_values\nFROM     nested_data"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["%sql\nSELECT   key,\n         values,\n         REDUCE(values, 0, (value, acc) -> value + acc, acc -> acc) summed_values,\n         REDUCE(values, 0, (value, acc) -> value + acc) summed_values_simple\nFROM     nested_data"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["%sql\nSELECT   key,\n         values,\n         AGGREGATE(values,\n           (1.0 AS product, 0 AS N),\n           (buffer, value) -> (value * buffer.product, buffer.N + 1),\n           buffer -> Power(buffer.product, 1.0 / buffer.N)) geomean\nFROM     nested_data"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["from pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\nschema = StructType() \\\n          .add(\"dc_id\", StringType()) \\\n          .add(\"source\", MapType(StringType(), StructType() \\\n                        .add(\"description\", StringType()) \\\n                        .add(\"ip\", StringType()) \\\n                        .add(\"id\", IntegerType()) \\\n                        .add(\"temp\", ArrayType(IntegerType())) \\\n                        .add(\"c02_level\", ArrayType(IntegerType())) \\\n                        .add(\"geo\", StructType() \\\n                              .add(\"lat\", DoubleType()) \\\n                              .add(\"long\", DoubleType()))))"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["def jsonToDataFrame(json, schema=None):\n  # SparkSessions are available with Spark 2.0+\n  reader = spark.read\n  if schema:\n    reader.schema(schema)\n  return reader.json(sc.parallelize([json]))"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["dataDF = jsonToDataFrame( \"\"\"{\n\n    \"dc_id\": \"dc-101\",\n    \"source\": {\n        \"sensor-igauge\": {\n        \"id\": 10,\n        \"ip\": \"68.28.91.22\",\n        \"description\": \"Sensor attached to the container ceilings\",\n        \"temp\":[35,35,35,36,35,35,32,35,30,35,32,35],\n        \"c02_level\": [1475,1476,1473],\n        \"geo\": {\"lat\":38.00, \"long\":97.00}                        \n      },\n      \"sensor-ipad\": {\n        \"id\": 13,\n        \"ip\": \"67.185.72.1\",\n        \"description\": \"Sensor ipad attached to carbon cylinders\",\n        \"temp\": [45,45,45,46,45,45,42,35,40,45,42,45],\n        \"c02_level\": [1370,1371,1372],\n        \"geo\": {\"lat\":47.41, \"long\":-122.00}\n      },\n      \"sensor-inest\": {\n        \"id\": 8,\n        \"ip\": \"208.109.163.218\",\n        \"description\": \"Sensor attached to the factory ceilings\",\n        \"temp\": [40,40,40,40,40,43,42,40,40,45,42,45],\n        \"c02_level\": [1346,1345, 1343],\n        \"geo\": {\"lat\":33.61, \"long\":-111.89}\n      },\n      \"sensor-istick\": {\n        \"id\": 5,\n        \"ip\": \"204.116.105.67\",\n        \"description\": \"Sensor embedded in exhaust pipes in the ceilings\",\n        \"temp\":[30,30,30,30,40,43,42,40,40,35,42,35],\n        \"c02_level\": [1574,1570, 1576],\n        \"geo\": {\"lat\":35.93, \"long\":-85.46}\n      }\n    }\n  }\"\"\", schema)\n\ndisplay(dataDF)"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["dataDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["explodedDF = dataDF.select(\"dc_id\", explode(\"source\"))\ndisplay(explodedDF)"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["devicesDataDF = explodedDF.select(\"dc_id\", \"key\", \\\n                        \"value.ip\", \\\n                        col(\"value.id\").alias(\"device_id\"), \\\n                        col(\"value.c02_level\").alias(\"c02_levels\"), \\\n                        \"value.temp\")\ndisplay(devicesDataDF)"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["devicesDataDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["devicesDataDF.createOrReplaceTempView(\"data_center_iot_devices\")"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["%sql select * from data_center_iot_devices"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["%sql describe data_center_iot_devices"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["%sql select key, ip, device_id, temp,\n     transform (temp, t -> ((t * 9) div 5) + 32 ) as fahrenheit_temp\n     from data_center_iot_devices"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["%sql select dc_id, key, ip, device_id, c02_levels, temp, \n     transform (c02_levels, t -> t > 1300) as high_c02_levels\n     from data_center_iot_devices"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":["%sql select dc_id, key, ip, device_id, c02_levels, temp, \n     filter (c02_levels, t -> t > 1300) as high_c02_levels\n     from data_center_iot_devices"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["%sql select dc_id, key, ip, device_id, c02_levels, temp, \n     filter (c02_levels, t -> t < 1300 ) as high_c02_levels\n     from data_center_iot_devices"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["%sql select dc_id, key, ip, device_id, c02_levels, temp, \n     exists (temp, t -> t = 45 ) as value_exists\n     from data_center_iot_devices"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["%sql select dc_id, key, ip, device_id, c02_levels, temp, \n     exists (c02_levels, t -> t = 1570 ) as high_c02_levels\n     from data_center_iot_devices"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["%sql select key, ip, device_id, temp,\n    reduce(temp, 0, (t, acc) -> t + acc, acc-> (acc div size(temp) * 9 div 5) + 32 ) as average_f_temp\n    from data_center_iot_devices\n    sort by average_f_temp desc"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["%sql select key, ip, device_id, c02_levels,\n    reduce(c02_levels, 0, (t, acc) -> t + acc, acc-> acc div size(c02_levels)) as average_c02_levels\n    from data_center_iot_devices\n    sort by  average_c02_levels desc\n"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["%sql select key, ip, device_id, c02_levels,\n     aggregate(c02_levels,\n               (1.0 as product, 0 as N),\n               (buffer, c02) -> (c02 * buffer.product, buffer.N+1),\n               buffer -> round(Power(buffer.product, 1.0 / buffer.N))) as c02_geomean\n     from data_center_iot_devices\n     sort by c02_geomean desc"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["schema2 = StructType() \\\n                    .add(\"device_id\", IntegerType()) \\\n                    .add(\"battery_level\", ArrayType(IntegerType())) \\\n                    .add(\"c02_level\", ArrayType(IntegerType())) \\\n                    .add(\"signal\", ArrayType(IntegerType())) \\\n                    .add(\"temp\", ArrayType(IntegerType())) \\\n                    .add(\"cca3\", ArrayType(StringType())) \\\n                    .add(\"device_type\", StringType()) \\\n                    .add(\"ip\", StringType()) \\\n                    .add(\"timestamp\", TimestampType())"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["dataDF2 = jsonToDataFrame(\"\"\"[\n  {\"device_id\": 0, \"device_type\": \"sensor-ipad\", \"ip\": \"68.161.225.1\", \"cca3\": [\"USA\", \"United States\"], \"temp\": [25,26, 27], \"signal\": [23,22,24], \"battery_level\": [8,9,7], \"c02_level\": [917, 921, 925], \"timestamp\" :1475600496 }, \n  {\"device_id\": 1, \"device_type\": \"sensor-igauge\", \"ip\": \"213.161.254.1\", \"cca3\": [\"NOR\", \"Norway\"], \"temp\": [30, 32,35], \"signal\": [18,18,19], \"battery_level\": [6, 6, 5], \"c02_level\": [1413, 1416, 1417], \"timestamp\" :1475600498 }, \n  {\"device_id\": 3, \"device_type\": \"sensor-inest\", \"ip\": \"66.39.173.154\", \"cca3\": [\"USA\", \"United States\"], \"temp\":[47, 47, 48], \"signal\": [12,12,13], \"battery_level\": [1, 1, 0],  \"c02_level\": [1447,1446, 1448], \"timestamp\" :1475600502 }, \n  {\"device_id\": 4, \"device_type\": \"sensor-ipad\", \"ip\": \"203.82.41.9\", \"cca3\":[\"PHL\", \"Philippines\"], \"temp\":[29, 29, 28], \"signal\":[11, 11, 11], \"battery_level\":[0, 0, 0], \"c02_level\": [983, 990, 982], \"timestamp\" :1475600504 },\n  {\"device_id\": 5, \"device_type\": \"sensor-istick\", \"ip\": \"204.116.105.67\", \"cca3\": [\"USA\", \"United States\"], \"temp\":[50,51,50], \"signal\": [16,16,17], \"battery_level\": [8,8, 8], \"c02_level\": [1574,1575,1576], \"timestamp\" :1475600506 }, \n  {\"device_id\": 6, \"device_type\": \"sensor-ipad\", \"ip\": \"220.173.179.1\", \"cca3\": [\"CHN\", \"China\"], \"temp\": [21,21,22], \"signal\": [18,18,19], \"battery_level\": [9,9,9], \"c02_level\": [1249,1249,1250], \"timestamp\" :1475600508 },\n  {\"device_id\": 7, \"device_type\": \"sensor-ipad\", \"ip\": \"118.23.68.227\", \"cca3\": [\"JPN\", \"Japan\"], \"temp\":[27,27,28], \"signal\": [15,15,29], \"battery_level\":[0,0,0], \"c02_level\": [1531,1532,1531], \"timestamp\" :1475600512 },\n  {\"device_id\": 8, \"device_type\": \"sensor-inest\", \"ip\": \"208.109.163.218\", \"cca3\": [\"USA\", \"United States\"], \"temp\":[40,40,41], \"signal\": [16,16,17], \"battery_level\":[ 9, 9, 10], \"c02_level\": [1208,1209,1208], \"timestamp\" :1475600514},\n  {\"device_id\": 9, \"device_type\": \"sensor-ipad\", \"ip\": \"88.213.191.34\", \"cca3\": [\"ITA\", \"Italy\"], \"temp\": [19,28,5], \"signal\": [11, 5, 24], \"battery_level\": [0,-1,0], \"c02_level\": [1171, 1240, 1400], \"timestamp\" :1475600516 },\n  {\"device_id\": 10, \"device_type\": \"sensor-igauge\", \"ip\": \"68.28.91.22\", \"cca3\": [\"USA\", \"United States\"], \"temp\": [32,33,32], \"signal\": [26,26,25], \"battery_level\": [7,7,8], \"c02_level\": [886,886,887], \"timestamp\" :1475600518 },\n  {\"device_id\": 11, \"device_type\": \"sensor-ipad\", \"ip\": \"59.144.114.250\", \"cca3\": [\"IND\", \"India\"], \"temp\": [46,45,44], \"signal\": [25,25,24], \"battery_level\": [4,5,5], \"c02_level\": [863,862,864], \"timestamp\" :1475600520 },\n  {\"device_id\": 12, \"device_type\": \"sensor-igauge\", \"ip\": \"193.156.90.200\", \"cca3\": [\"NOR\", \"Norway\"], \"temp\": [18,17,18], \"signal\": [26,25,26], \"battery_level\": [8,9,8], \"c02_level\": [1220,1221,1220], \"timestamp\" :1475600522 },\n  {\"device_id\": 13, \"device_type\": \"sensor-ipad\", \"ip\": \"67.185.72.1\", \"cca3\": [\"USA\", \"United States\"], \"temp\": [34,35,34], \"signal\": [20,21,20], \"battery_level\": [8,8,8], \"c02_level\": [1504,1504,1503], \"timestamp\" :1475600524 },\n  {\"device_id\": 14, \"device_type\": \"sensor-inest\", \"ip\": \"68.85.85.106\", \"cca3\": [\"USA\", \"United States\"], \"temp\": [39,40,38], \"signal\": [17, 17, 18], \"battery_level\": [8,8,7], \"c02_level\": [831,832,831], \"timestamp\" :1475600526 },\n  {\"device_id\": 15, \"device_type\": \"sensor-ipad\", \"ip\": \"161.188.212.254\", \"cca3\": [\"USA\", \"United States\"], \"temp\": [27,27,28], \"signal\": [26,26,25], \"battery_level\": [5,5,5], \"c02_level\": [1378,1376,1378], \"timestamp\" :1475600528 },\n  {\"device_id\": 16, \"device_type\": \"sensor-igauge\", \"ip\": \"221.3.128.242\", \"cca3\": [\"CHN\", \"China\"], \"temp\": [10,10,11], \"signal\": [24,24,23], \"battery_level\": [6,5,6], \"c02_level\": [1423, 1423, 1423], \"timestamp\" :1475600530 },\n  {\"device_id\": 17, \"device_type\": \"sensor-ipad\", \"ip\": \"64.124.180.215\", \"cca3\": [\"USA\", \"United States\"], \"temp\": [38,38,39], \"signal\": [17,17,17], \"battery_level\": [9,9,9], \"c02_level\": [1304,1304,1304], \"timestamp\" :1475600532 },\n  {\"device_id\": 18, \"device_type\": \"sensor-igauge\", \"ip\": \"66.153.162.66\", \"cca3\": [\"USA\", \"United States\"], \"temp\": [26, 0, 99], \"signal\": [10, 1, 5], \"battery_level\": [0, 0, 0], \"c02_level\": [902,902, 1300], \"timestamp\" :1475600534 },\n  {\"device_id\": 19, \"device_type\": \"sensor-ipad\", \"ip\": \"193.200.142.254\", \"cca3\": [\"AUT\", \"Austria\"], \"temp\": [32,32,33], \"signal\": [27,27,28], \"battery_level\": [5,5,5], \"c02_level\": [1282, 1282, 1281], \"timestamp\" :1475600536 }\n  ]\"\"\", schema2)\n\ndisplay(dataDF2)"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"code","source":["dataDF2.printSchema()"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["dataDF2.createOrReplaceTempView(\"iot_nested_data\")"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["%sql select cca3, device_type, battery_level,\n     transform (battery_level, bl -> bl > 0) as boolean_battery_level\n     from iot_nested_data"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["%sql select cca3,\n     transform (cca3, c -> lcase(c)) as lower_cca3,\n     transform (cca3, c -> ucase(c)) as upper_cca3\n     from iot_nested_data"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"code","source":["%sql select cca3, device_type, battery_level,\n     filter (battery_level, bl -> bl < 5) as low_levels\n     from iot_nested_data"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["%sql select cca3, device_type, battery_level,\n     reduce(battery_level, 0, (t, acc) -> t + acc,  acc -> acc div size(battery_level) ) as average_battery_level\n     from iot_nested_data\n     sort by average_battery_level desc"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["%sql select cca3, device_type, temp,\n     reduce(temp, 0, (t, acc) -> t + acc,  acc -> acc div size(temp) ) as average_temp\n     from iot_nested_data\n     sort by average_temp desc"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["%sql select cca3, device_type, c02_level,\n     reduce(c02_level, 0, (t, acc) -> t + acc,  acc -> acc div size(c02_level) ) as average_c02_level\n     from iot_nested_data\n     sort by average_c02_level desc"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"code","source":["%sql select cca3, device_type, signal, temp, c02_level,\n     reduce(signal, 0, (s, sacc) -> s + sacc,  sacc -> sacc div size(signal) ) as average_signal,\n     reduce(temp, 0, (t, tacc) -> t + tacc,  tacc -> tacc div size(temp) ) as average_temp,\n     reduce(c02_level, 0, (c, cacc) -> c + cacc,  cacc -> cacc div size(c02_level) ) as average_c02_level\n     from iot_nested_data\n     sort by average_signal desc"],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.expressions.MutableAggregationBuffer\nimport org.apache.spark.sql.expressions.UserDefinedAggregateFunction\nimport org.apache.spark.sql.Row\nimport org.apache.spark.sql.types._\n\nclass GeometricMean extends UserDefinedAggregateFunction {\n  // This is the input fields for your aggregate function.\n  override def inputSchema: org.apache.spark.sql.types.StructType =\n    StructType(StructField(\"value\", DoubleType) :: Nil)\n\n  // This is the internal fields you keep for computing your aggregate.\n  override def bufferSchema: StructType = StructType(\n    StructField(\"count\", LongType) ::\n    StructField(\"product\", DoubleType) :: Nil\n  )\n\n  // This is the output type of your aggregatation function.\n  override def dataType: DataType = DoubleType\n\n  override def deterministic: Boolean = true\n\n  // This is the initial value for your buffer schema.\n  override def initialize(buffer: MutableAggregationBuffer): Unit = {\n    buffer(0) = 0L\n    buffer(1) = 1.0\n  }\n\n  // This is how to update your buffer schema given an input.\n  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n    buffer(0) = buffer.getAs[Long](0) + 1\n    buffer(1) = buffer.getAs[Double](1) * input.getAs[Double](0)\n  }\n\n  // This is how to merge two objects with the bufferSchema type.\n  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n    buffer1(0) = buffer1.getAs[Long](0) + buffer2.getAs[Long](0)\n    buffer1(1) = buffer1.getAs[Double](1) * buffer2.getAs[Double](1)\n  }\n\n  // This is where you output the final value, given the final value of your bufferSchema.\n  override def evaluate(buffer: Row): Any = {\n    math.pow(buffer.getDouble(1), 1.toDouble / buffer.getLong(0))\n  }\n}"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["%scala\nsqlContext.udf.register(\"gm\", new GeometricMean)"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"code","source":["%scala\n// Create a DataFrame and Spark SQL Table to query.\nimport org.apache.spark.sql.functions._\n\nval ids = sqlContext.range(1, 20)\nids.registerTempTable(\"ids\")\nval df = sqlContext.sql(\"select id, id % 3 as group_id from ids\")\ndf.registerTempTable(\"simple\")"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"code","source":["%sql\n-- Use a group_by statement and call the UDAF.\nselect group_id, gm(id) from simple group by group_id"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"code","source":["%scala\n// Or use Dataframe syntax to call the aggregate function.\n\n// Create an instance of UDAF GeometricMean.\nval gm = new GeometricMean\n\n// Show the geometric mean of values of column \"id\".\ndf.groupBy(\"group_id\").agg(gm(col(\"id\")).as(\"GeometricMean\")).show()\n\n// Invoke the UDAF by its assigned name.\ndf.groupBy(\"group_id\").agg(expr(\"gm(id) as GeometricMean\")).show()"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"code","source":["def squared(s):\n  return s * s\nsqlContext.udf.register(\"squaredWithPython\", squared)"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"code","source":["from pyspark.sql.types import LongType\ndef squared_typed(s):\n  return s * s\nsqlContext.udf.register(\"squaredWithPython\", squared, LongType())"],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"code","source":["sqlContext.range(1, 20).registerTempTable(\"test\")"],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"code","source":["%sql select id, squaredWithPython(id) as id_squared from test"],"metadata":{},"outputs":[],"execution_count":113},{"cell_type":"code","source":["%sql select * FROM test"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nsquared_udf = udf(squared, LongType())\ndf = sqlContext.table(\"test\")\ndisplay(df.select(\"id\", squared_udf(\"id\").alias(\"id_squared\")))"],"metadata":{},"outputs":[],"execution_count":115},{"cell_type":"code","source":["%scala\nval squared = (s: Int) => {\n  s * s\n}\nsqlContext.udf.register(\"square\", squared)"],"metadata":{},"outputs":[],"execution_count":116},{"cell_type":"code","source":["%scala\nsqlContext.range(1, 20).registerTempTable(\"test\")"],"metadata":{},"outputs":[],"execution_count":117},{"cell_type":"code","source":["%sql select id, square(id) as id_squared from test"],"metadata":{},"outputs":[],"execution_count":118},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":119}],"metadata":{"name":"databricks docs 6 (transform sparksql","notebookId":3117922773650486},"nbformat":4,"nbformat_minor":0}
