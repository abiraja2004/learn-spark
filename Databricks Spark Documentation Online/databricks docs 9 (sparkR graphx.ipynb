{"cells":[{"cell_type":"code","source":["%r\nlibrary(SparkR)\ndf <- createDataFrame(faithful)\n\n# Displays the content of the DataFrame to stdout\nhead(df)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%r\nlibrary(SparkR)\ndiamondsDF <- read.df(\"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\",\n                    source = \"csv\", header=\"true\", inferSchema = \"true\")\nhead(diamonds)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%r\nprintSchema(diamondsDF)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%r\ndisplay(diamondsDF)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%r\nwrite.df(irisDF2, path=\"dbfs:/tmp/iris.parquet\", source=\"parquet\", mode=\"overwrite\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%fs ls dbfs:/tmp/people.parquet"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%r\n# Register earlier df as temp table\nregisterTempTable(people, \"peopleTemp\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%r\nage <- sql(\"SELECT age FROM peopleTemp\")\nhead(age)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["%r\nrequire(SparkR)\n\n# Create DataFrame\ndf <- createDataFrame(faithful)\ndf"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["%r\n# Select only the \"eruptions\" column\nhead(select(df, df$eruptions))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["%r\n# You can also pass in column name as strings\nhead(select(df, \"eruptions\"))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%r\n# Filter the DataFrame to only retain rows with wait times shorter than 50 mins\nhead(filter(df, df$waiting < 50))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%r\nhead(count(groupBy(df, df$waiting)))"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["%r\n# We can also sort the output from the aggregation to get the most common waiting times\nwaiting_counts <- count(groupBy(df, df$waiting))\nhead(arrange(waiting_counts, desc(waiting_counts$count)))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%r\n# Convert waiting time from hours to seconds.\n# Note that we can assign this to a new column in the same DataFrame\ndf$waiting_secs <- df$waiting * 60\nhead(df)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%r\n# Create the DataFrame\ndf <- createDataFrame(sqlContext, iris)\n\n# Fit a linear model over the dataset.\nmodel <- glm(Sepal_Length ~ Sepal_Width + Species, data = df, family = \"gaussian\")\n\n# Model coefficients are returned in a similar format to R's native glm().\nsummary(model)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%r\nrequire(SparkR)\n\n# Read diamonds.csv dataset as SparkDataFrame\ndiamonds <- read.df(\"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\",\n                  source = \"com.databricks.spark.csv\", header=\"true\", inferSchema = \"true\")\ndiamonds <- withColumnRenamed(diamonds, \"\", \"rowID\")\n\n# Split data into Training set and Test set\ntrainingData <- sample(diamonds, FALSE, 0.7)\ntestData <- except(diamonds, trainingData)\n\n# Exclude rowIDs\ntrainingData <- trainingData[, -1]\ntestData <- testData[, -1]\n\nprint(count(diamonds))\nprint(count(trainingData))\nprint(count(testData))"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["%r\nhead(trainingData)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%r\n# Indicate family = \"gaussian\" to train a linear regression model\nlrModel <- glm(price ~ ., data = trainingData, family = \"gaussian\")\n\n# Print a summary of trained linear regression model\nsummary(lrModel)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%r\n# Generate predictions using the trained Linear Regression model\npredictions <- predict(lrModel, newData = testData)\n\n# View predictions against mpg column\ndisplay(select(predictions, \"price\", \"prediction\"))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["%r\nerrors <- select(predictions, predictions$price, predictions$prediction, alias(predictions$price - predictions$prediction, \"error\"))\ndisplay(errors)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["%r\n# Calculate RMSE\nhead(select(errors, alias(sqrt(sum(errors$error^2 , na.rm = TRUE) / nrow(errors)), \"RMSE\")))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["%r\n# Subset data to include rows where diamond cut = \"Premium\" or diamond cut = \"Very Good\"\ntrainingDataSub <- subset(trainingData, trainingData$cut %in% c(\"Premium\", \"Very Good\"))\ntestDataSub <- subset(testData, testData$cut %in% c(\"Premium\", \"Very Good\"))"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["%r\n# Indicate family = \"binomial\" to train a logistic regression model\nlogrModel <- glm(cut ~ price + color + clarity + depth, data = trainingDataSub, family = \"binomial\")\n\n# Print summary of Logistic Regression model\n# Note: This only works in Spark 1.6+\nsummary(logrModel)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["%r\n# Generate predictions using the trained Linear Regression model\npredictionsLogR <- predict(logrModel, newData = testDataSub)\n\n# View predictions against label column\ndisplay(select(predictionsLogR, \"label\", \"prediction\"))"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["%r\n# Evaluate Logistic Regression model\nerrorsLogR <- select(predictionsLogR, predictionsLogR$label, predictionsLogR$prediction, alias(abs(predictionsLogR$label - predictionsLogR$prediction), \"error\"))\ndisplay(errorsLogR)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%scala\nval bikeStations = sqlContext.sql(\"SELECT * FROM sf_201508_station_data\")\nval tripData = sqlContext.sql(\"SELECT * FROM sf_201508_trip_data\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["%scala\ndisplay(bikeStations)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["%scala\ndisplay(tripData)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["%scala\nbikeStations.printSchema()\ntripData.printSchema()"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["%scala\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["%scala\nval justStations = bikeStations\n  .selectExpr(\"float(station_id) as station_id\", \"name\")\n  .distinct()\n\nval completeTripData = tripData\n  .join(justStations, tripData(\"Start Station\") === bikeStations(\"name\"))\n  .withColumnRenamed(\"station_id\", \"start_station_id\")\n  .drop(\"name\")\n  .join(justStations, tripData(\"End Station\") === bikeStations(\"name\"))\n  .withColumnRenamed(\"station_id\", \"end_station_id\")\n  .drop(\"name\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["%scala\nval stations = completeTripData\n  .select(\"start_station_id\", \"end_station_id\")\n  .rdd\n  .distinct() // helps filter out duplicate trips\n  .flatMap(x => Iterable(x(0).asInstanceOf[Number].longValue, x(1).asInstanceOf[Number].longValue)) // helps us maintain types\n  .distinct()\n  .toDF() // return to a DF to make merging + joining easier\n\nstations.take(1) // this is just a station_id at this point"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["%scala\nval stationVertices: RDD[(VertexId, String)] = stations\n  .join(justStations, stations(\"value\") === justStations(\"station_id\"))\n  .select(\"station_id\", \"name\")\n  .rdd\n  .map(row => (row(0).asInstanceOf[Number].longValue, row(1).asInstanceOf[String])) // maintain type information\n\nstationVertices.take(1)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["%scala\nval stationEdges:RDD[Edge[Long]] = completeTripData\n  .select(\"start_station_id\", \"end_station_id\")\n  .rdd\n  .map(row => Edge(row(0).asInstanceOf[Number].longValue, row(1).asInstanceOf[Number].longValue, 1))"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["%scala\nval defaultStation = (\"Missing Station\")\nval stationGraph = Graph(stationVertices, stationEdges, defaultStation)\nstationGraph.cache()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["%scala\nprintln(\"Total Number of Stations: \" + stationGraph.numVertices)\nprintln(\"Total Number of Trips: \" + stationGraph.numEdges)\n// sanity check\nprintln(\"Total Number of Trips in Original Data: \" + tripData.count)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["%scala\nval ranks = stationGraph.pageRank(0.0001).vertices\nranks\n  .join(stationVertices)\n  .sortBy(_._2._1, ascending=false) // sort by the rank\n  .take(10) // get the top 10\n  .foreach(x => println(x._2._2))"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["%scala\nstationGraph\n  .groupEdges((edge1, edge2) => edge1 + edge2)\n  .triplets\n  .sortBy(_.attr, ascending=false)\n  .map(triplet =>\n    \"There were \" + triplet.attr.toString + \" trips from \" + triplet.srcAttr + \" to \" + triplet.dstAttr + \".\")\n  .take(10)\n  .foreach(println)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["%scala\nstationGraph\n  .inDegrees // computes in Degrees\n  .join(stationVertices)\n  .sortBy(_._2._1, ascending=false)\n  .take(10)\n  .foreach(x => println(x._2._2 + \" has \" + x._2._1 + \" in degrees.\"))"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["%scala\nstationGraph\n  .outDegrees // out degrees\n  .join(stationVertices)\n  .sortBy(_._2._1, ascending=false)\n  .take(10)\n  .foreach(x => println(x._2._2 + \" has \" + x._2._1 + \" out degrees.\"))"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["%scala\nstationGraph\n  .inDegrees\n  .join(stationGraph.outDegrees) // join with out Degrees\n  .join(stationVertices) // join with our other stations\n  .map(x => (x._2._1._1.toDouble/x._2._1._2.toDouble, x._2._2)) // ratio of in to out\n  .sortBy(_._1, ascending=false)\n  .take(5)\n  .foreach(x => println(x._2 + \" has a in/out degree ratio of \" + x._1))"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["%scala\nstationGraph\n  .inDegrees\n  .join(stationGraph.inDegrees) // join with out Degrees\n  .join(stationVertices) // join with our other stations\n  .map(x => (x._2._1._1.toDouble/x._2._1._2.toDouble, x._2._2)) // ratio of in to out\n  .sortBy(_._1)\n  .take(5)\n  .foreach(x => println(x._2 + \" has a in/out degree ratio of \" + x._1))"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["%sh --packages graphframes:graphframes:0.5.0-spark2.1-s_2.11"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.graphframes._"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["%scala\nval stationVertices = bikeStations\n  .withColumnRenamed(\"name\", \"id\")\n  .distinct()\n\nval tripEdges = tripData\n  .withColumnRenamed(\"Start Station\", \"src\")\n  .withColumnRenamed(\"End Station\", \"dst\")"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["%scala\ndisplay(stationVertices)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["%scala\ndisplay(tripEdges)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["%scala\nval stationGraph = GraphFrame(stationVertices, tripEdges)\n\ntripEdges.cache()\nstationVertices.cache()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["%scala\nval ranks = stationGraph.pageRank.resetProbability(0.15).maxIter(10).run()\n\ndisplay(ranks.vertices.orderBy(desc(\"pagerank\")))"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["%scala\nval inDeg = stationGraph.inDegrees\ndisplay(inDeg.orderBy(desc(\"inDegree\")).limit(5))"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["%scala\nval degreeRatio = inDeg.join(outDeg, inDeg.col(\"id\") === outDeg.col(\"id\"))\n  .drop(outDeg.col(\"id\"))\n  .selectExpr(\"id\", \"double(inDegree)/double(outDegree) as degreeRatio\")\n\ndegreeRatio.cache()\n  \ndisplay(degreeRatio.orderBy(desc(\"degreeRatio\")).limit(10))"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["from graphframes import *"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["vertices = sqlContext.createDataFrame([\n  (\"a\", \"Alice\", 34),\n  (\"b\", \"Bob\", 36),\n  (\"c\", \"Charlie\", 30),\n  (\"d\", \"David\", 29),\n  (\"e\", \"Esther\", 32),\n  (\"f\", \"Fanny\", 36),\n  (\"g\", \"Gabby\", 60)], [\"id\", \"name\", \"age\"])"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["edges = sqlContext.createDataFrame([\n  (\"a\", \"b\", \"friend\"),\n  (\"b\", \"c\", \"follow\"),\n  (\"c\", \"b\", \"follow\"),\n  (\"f\", \"c\", \"follow\"),\n  (\"e\", \"f\", \"follow\"),\n  (\"e\", \"d\", \"friend\"),\n  (\"d\", \"a\", \"friend\"),\n  (\"a\", \"e\", \"friend\")\n], [\"src\", \"dst\", \"relationship\"])"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["g = GraphFrame(vertices, edges)\nprint g"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["display(g.vertices)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["display(g.edges)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["display(g.inDegrees)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["display(g.outDegrees)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["display(g.degrees)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["youngest = g.vertices.groupBy().min(\"age\")\ndisplay(youngest)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"code","source":["numFollows = g.edges.filter(\"relationship = 'follow'\").count()\nprint \"The number of follow edges is\", numFollows"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"code","source":["# Search for pairs of vertices with edges in both directions between them.\nmotifs = g.find(\"(a)-[e]->(b); (b)-[e2]->(a)\")\ndisplay(motifs)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["filtered = motifs.filter(\"b.age > 30 or a.age > 30\")\ndisplay(filtered)"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["paths = g.find(\"(a)-[e]->(b)\")\\\n  .filter(\"e.relationship = 'follow'\")\\\n  .filter(\"a.age < b.age\")\n# The `paths` variable contains the vertex information, which we can extract:\ne2 = paths.select(\"e.src\", \"e.dst\", \"e.relationship\")\n\n# In Spark 1.5+, the user may simplify the previous call to:\n# val e2 = paths.select(\"e.*\")\n\n# Construct the subgraph\ng2 = GraphFrame(g.vertices, e2)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["display(g2.vertices)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["%scala\n\nval firstDF = spark.range(3).toDF(\"myCol\")\nval newRow = Seq(20)\nval appended = firstDF.union(newRow.toDF())\ndisplay(appended)"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["firstDF = spark.range(3).toDF(\"myCol\")\nnewRow = spark.createDataFrame([[20]])\nappended = firstDF.union(newRow)\ndisplay(appended)"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["%scala\n\nval llist = Seq((\"bob\", \"2015-01-13\", 4), (\"alice\", \"2015-04-23\",10))\nval left = llist.toDF(\"name\",\"date\",\"duration\")\nval right = Seq((\"alice\", 100),(\"bob\", 23)).toDF(\"name\",\"upload\")\n\nval df = left.join(right, left.col(\"name\") === right.col(\"name\"))\n\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["%scala\n\nval df = left.join(right, Seq(\"name\"))\n display(df)"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["%scala\nsc.parallelize(Seq(\"\")).foreachPartition(x => {\n  import org.apache.log4j.{LogManager, Level}\n  import org.apache.commons.logging.LogFactory\n\n  LogManager.getRootLogger().setLevel(Level.DEBUG)\n  val log = LogFactory.getLog(\"EXECUTOR-LOG:\")\n  log.debug(\"START EXECUTOR DEBUG LOG LEVEL\")\n})"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["%sh \ncurl -O http://download.tensorflow.org/example_images/flower_photos.tgz\ntar xzf flower_photos.tgz"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["display(dbutils.fs.ls('file:/databricks/driver/flower_photos'))"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["# The 'file:/...' directory will be cleared out upon cluster termination. That doesn't matter for this example notebook, but in most cases we'd want to store the images in a more permanent place. Let's move the files to dbfs so we can see how to work with it in the use cases below.\nimg_dir = '/tmp/flower_photos'\ndbutils.fs.mkdirs(img_dir)\ndbutils.fs.cp('file:/databricks/driver/flower_photos/tulips', img_dir + \"/tulips\", recurse=True)\ndbutils.fs.cp('file:/databricks/driver/flower_photos/daisy', img_dir + \"/daisy\", recurse=True)\ndbutils.fs.cp('file:/databricks/driver/flower_photos/LICENSE.txt', img_dir)\ndisplay(dbutils.fs.ls(img_dir))"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["# Let's create a small sample set of images for quick demonstrations.\nsample_img_dir = img_dir + \"/sample\"\ndbutils.fs.mkdirs(sample_img_dir)\nfiles = dbutils.fs.ls(img_dir + \"/tulips\")[0:1] + dbutils.fs.ls(img_dir + \"/daisy\")[0:2]\nfor f in files:\n  dbutils.fs.cp(f.path, sample_img_dir)\ndisplay(dbutils.fs.ls(sample_img_dir))"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["from sparkdl import readImages\nimage_df = readImages(sample_img_dir)"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["display(image_df)"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["# Create training & test DataFrames for transfer learning - this piece of code is longer than transfer learning itself below!\nfrom sparkdl import readImages\nfrom pyspark.sql.functions import lit\n\ntulips_df = readImages(img_dir + \"/tulips\").withColumn(\"label\", lit(1))\ndaisy_df = readImages(img_dir + \"/daisy\").withColumn(\"label\", lit(0))\ntulips_train, tulips_test = tulips_df.randomSplit([0.6, 0.4])\ndaisy_train, daisy_test = daisy_df.randomSplit([0.6, 0.4])\ntrain_df = tulips_train.unionAll(daisy_train)\ntest_df = tulips_test.unionAll(daisy_test)"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline\nfrom sparkdl import DeepImageFeaturizer \n\nfeaturizer = DeepImageFeaturizer(inputCol=\"image\", outputCol=\"features\", modelName=\"InceptionV3\")\nlr = LogisticRegression(maxIter=20, regParam=0.05, elasticNetParam=0.3, labelCol=\"label\")\np = Pipeline(stages=[featurizer, lr])\n\np_model = p.fit(train_df)"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"code","source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\ntested_df = p_model.transform(test_df)\nevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\nprint(\"Test set accuracy = \" + str(evaluator.evaluate(tested_df.select(\"prediction\", \"label\"))))"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["from pyspark.sql.types import DoubleType\nfrom pyspark.sql.functions import expr\ndef _p1(v):\n  return float(v.array[1])\np1 = udf(_p1, DoubleType())\n\ndf = tested_df.withColumn(\"p_1\", p1(tested_df.probability))\nwrong_df = df.orderBy(expr(\"abs(p_1 - label)\"), ascending=False)\ndisplay(wrong_df.select(\"filePath\", \"p_1\", \"label\").limit(10))"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["from sparkdl import readImages, DeepImagePredictor\n\nimage_df = readImages(sample_img_dir)\n\npredictor = DeepImagePredictor(inputCol=\"image\", outputCol=\"predicted_labels\", modelName=\"InceptionV3\", decodePredictions=True, topK=10)\npredictions_df = predictor.transform(image_df)\n\ndisplay(predictions_df.select(\"filePath\", \"predicted_labels\"))"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["df = p_model.transform(image_df)\ndisplay(df.select(\"filePath\", (1-p1(df.probability)).alias(\"p_daisy\")))"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["from sparkdl import readImages, TFImageTransformer\nfrom sparkdl.transformers import utils\nimport tensorflow as tf\n\nimage_df = readImages(sample_img_dir)\n\ng = tf.Graph()\nwith g.as_default():\n    image_arr = utils.imageInputPlaceholder()\n    resized_images = tf.image.resize_images(image_arr, (299, 299))\n    # the following step is not necessary for this graph, but can be for graphs with variables, etc\n    frozen_graph = utils.stripAndFreezeGraph(g.as_graph_def(add_shapes=True), tf.Session(graph=g), [resized_images])\n      \ntransformer = TFImageTransformer(inputCol=\"image\", outputCol=\"transformed_img\", graph=frozen_graph,\n                                 inputTensor=image_arr, outputTensor=resized_images,\n                                 outputMode=\"image\")\ntf_trans_df = transformer.transform(image_df)"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["from keras.applications import InceptionV3\n\nmodel = InceptionV3(weights=\"imagenet\")\nmodel.save('/tmp/model-full.h5')  # saves to the local filesystem\n# move to a permanent place for future use\ndbfs_model_path = 'dbfs:/models/model-full.h5'\ndbutils.fs.cp('file:/tmp/model-full.h5', dbfs_model_path) "],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":["from keras.applications.inception_v3 import preprocess_input\nfrom keras.preprocessing.image import img_to_array, load_img\nimport numpy as np\nfrom pyspark.sql.types import StringType\nfrom sparkdl import KerasImageFileTransformer\n\ndef loadAndPreprocessKerasInceptionV3(uri):\n  # this is a typical way to load and prep images in keras\n  image = img_to_array(load_img(uri, target_size=(299, 299)))  # image dimensions for InceptionV3\n  image = np.expand_dims(image, axis=0)\n  return preprocess_input(image)\n\ndbutils.fs.cp(dbfs_model_path, 'file:/tmp/model-full-tmp.h5')\ntransformer = KerasImageFileTransformer(inputCol=\"uri\", outputCol=\"predictions\",\n                                        modelFile='/tmp/model-full-tmp.h5',  # local file path for model\n                                        imageLoader=loadAndPreprocessKerasInceptionV3,\n                                        outputMode=\"vector\")\n\nfiles = [\"/dbfs\" + str(f.path)[5:] for f in dbutils.fs.ls(sample_img_dir)]  # make \"local\" file paths for images\nuri_df = sqlContext.createDataFrame(files, StringType()).toDF(\"uri\")\n\nkeras_pred_df = transformer.transform(uri_df)"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"code","source":["display(keras_pred_df.select(\"uri\", \"predictions\"))"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["dbutils.fs.rm(img_dir, recurse=True)\ndbutils.fs.rm(dbfs_model_path)"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["import tensorflow as tf\ntf.__version__"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\n\n# Import data\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport tensorflow as tf"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["mnist = input_data.read_data_sets('/tmp/data', one_hot=True)"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["x = tf.placeholder(tf.float32, [None, 784])\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\ny = tf.matmul(x, W) + b"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["y_ = tf.placeholder(tf.float32, [None, 10])"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\ntrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"code","source":["correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nsummary = tf.summary.scalar(\"accuracy\", accuracy)"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["log_dir = \"/tmp/tensorflow_log_dir\"\ndbutils.tensorboard.start(log_dir)"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["sess = tf.InteractiveSession()\n\n# Make sure to use the same log directory for both start TensorBoard in your training.\nsummary_writer = tf.summary.FileWriter(log_dir, graph=sess.graph)\n\ntf.global_variables_initializer().run()\nfor batch in range(1000):\n  batch_xs, batch_ys = mnist.train.next_batch(100)\n  _, batch_summary = sess.run([train_step, summary], feed_dict={x: batch_xs, y_: batch_ys})\n  summary_writer.add_summary(batch_summary, batch)"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["print(sess.run(accuracy, feed_dict={x: mnist.test.images,\n                                    y_: mnist.test.labels}))"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"code","source":["dbutils.tensorboard.stop()"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["import shutil\nshutil.move(log_dir, \"/dbfs/tensorflow/logs\")"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["import numpy as np\nimport os\nimport urllib\nimport gzip\nimport struct\ndef download_data(url, force_download=True): \n    fname = url.split(\"/\")[-1]\n    if force_download or not os.path.exists(fname):\n        urllib.urlretrieve(url, fname)\n    return fname\n\ndef read_data(label_url, image_url):\n    with gzip.open(download_data(label_url)) as flbl:\n        magic, num = struct.unpack(\">II\", flbl.read(8))\n        label = np.fromstring(flbl.read(), dtype=np.int8)\n    with gzip.open(download_data(image_url), 'rb') as fimg:\n        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n        image = np.fromstring(fimg.read(), dtype=np.uint8).reshape(len(label), rows, cols)\n    return (label, image)\n\npath='http://yann.lecun.com/exdb/mnist/'\n(train_lbl, train_img) = read_data(\n    path+'train-labels-idx1-ubyte.gz', path+'train-images-idx3-ubyte.gz')\n(val_lbl, val_img) = read_data(\n    path+'t10k-labels-idx1-ubyte.gz', path+'t10k-images-idx3-ubyte.gz')"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nfor i in range(10):\n    plt.subplot(1,10,i+1)\n    plt.imshow(train_img[i], cmap='Greys_r')\n    plt.axis('off')\nplt.show()\ndisplay()"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"code","source":["print('label: %s' % (train_lbl[0:10],))"],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"code","source":["import mxnet as mx\n\ndef to4d(img):\n    return img.reshape(img.shape[0], 1, 28, 28).astype(np.float32)/255"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":["# Create a place holder variable for the input data\ndata = mx.sym.Variable('data')\n# Flatten the data from 4-D shape (batch_size, num_channel, width, height) \n# into 2-D (batch_size, num_channel*width*height)\ndata = mx.sym.Flatten(data=data)\n\n# The first fully-connected layer\nfc1  = mx.sym.FullyConnected(data=data, name='fc1', num_hidden=128)\n# Apply relu to the output of the first fully-connnected layer\nact1 = mx.sym.Activation(data=fc1, name='relu1', act_type=\"relu\")\n\n# The second fully-connected layer and the according activation function\nfc2  = mx.sym.FullyConnected(data=act1, name='fc2', num_hidden = 64)\nact2 = mx.sym.Activation(data=fc2, name='relu2', act_type=\"relu\")\n\n# The thrid fully-connected layer, note that the hidden size should be 10, which is the number of unique digits\nfc3  = mx.sym.FullyConnected(data=act2, name='fc3', num_hidden=10)\n# The softmax and loss layer\nmlp  = mx.sym.SoftmaxOutput(data=fc3, name='softmax')"],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"code","source":["model = mx.model.FeedForward(\n    symbol = mlp,       # network structure\n    num_epoch = 10,     # number of data passes for training \n    learning_rate = 0.1 # learning rate of SGD \n)\nmodel.fit(\n    X=train_iter,       # training data\n    eval_data=val_iter, # validation data\n    batch_end_callback = mx.callback.Speedometer(batch_size, 200) # output progress for each 200 data batches\n) "],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"code","source":["plt.clf()\nplt.imshow(val_img[0], cmap='Greys_r')\nplt.axis('off')\nplt.show()\ndisplay()"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"code","source":["prob = model.predict(val_img[0:1].astype(np.float32)/255)[0]\nprint 'Classified as %d with probability %f' % (prob.argmax(), max(prob))"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"code","source":["print 'Validation accuracy: %f%%' % (model.score(val_iter)*100,)"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"code","source":["data = mx.symbol.Variable('data')\n# first conv layer\nconv1 = mx.sym.Convolution(data=data, kernel=(5,5), num_filter=20)\ntanh1 = mx.sym.Activation(data=conv1, act_type=\"tanh\")\npool1 = mx.sym.Pooling(data=tanh1, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n# second conv layer\nconv2 = mx.sym.Convolution(data=pool1, kernel=(5,5), num_filter=50)\ntanh2 = mx.sym.Activation(data=conv2, act_type=\"tanh\")\npool2 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n# first fullc layer\nflatten = mx.sym.Flatten(data=pool2)\nfc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\ntanh3 = mx.sym.Activation(data=fc1, act_type=\"tanh\")\n# second fullc\nfc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=10)\n# softmax loss\nlenet = mx.sym.SoftmaxOutput(data=fc2, name='softmax')"],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"code","source":["model = mx.model.FeedForward(\n    ctx = mx.gpu(0),     # use GPU 0 for training, others are same as before\n    symbol = lenet,       \n    num_epoch = 10,     \n    learning_rate = 0.1)\nmodel.fit(\n    X=train_iter,  \n    eval_data=val_iter, \n    batch_end_callback = mx.callback.Speedometer(batch_size, 200)\n) "],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"code","source":["print 'Validation accuracy: %f%%' % (model.score(val_iter)*100,)"],"metadata":{},"outputs":[],"execution_count":113},{"cell_type":"code","source":["from __future__ import print_function\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K"],"metadata":{},"outputs":[],"execution_count":114},{"cell_type":"code","source":["# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()"],"metadata":{},"outputs":[],"execution_count":115},{"cell_type":"code","source":["# input image dimensions\nimg_rows, img_cols = 28, 28\n# number of classes (digits) to predict\nnum_classes = 10\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)"],"metadata":{},"outputs":[],"execution_count":116},{"cell_type":"code","source":["model = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])"],"metadata":{},"outputs":[],"execution_count":117},{"cell_type":"code","source":["batch_size = 128\nepochs = 12\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))"],"metadata":{},"outputs":[],"execution_count":118},{"cell_type":"code","source":["score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])"],"metadata":{},"outputs":[],"execution_count":119},{"cell_type":"code","source":["# name of the cluster this script will be applied to\nclusterName = \"caffe-gpu\"\n\n# Caffe git hash/tag. Other hashes/tags might work, but only this specific tag was tested.\ncaffeGitTag = \"48e73c7\""],"metadata":{},"outputs":[],"execution_count":120},{"cell_type":"code","source":["script = \"\"\"#!/usr/bin/env bash\n\nset -ex\n\necho \"**** Installing Caffe dependencies ****\"\n\napt-get update\n\n# Requirements stated in Caffe docs.\napt-get -y install libleveldb-dev libopencv-dev libhdf5-serial-dev protobuf-compiler libatlas-base-dev\napt-get -y install --no-install-recommends libboost-all-dev\n\n# Additional requirements on Ubuntu 16.04.\napt-get -y install libgflags-dev libprotobuf-dev libgoogle-glog-dev liblmdb-dev\n\n# The following should be installed already. Keep them to make the script self-contained.\napt-get -y install python-dev libsnappy-dev python-pip git-all gcc\n\necho \"**** Downloading Caffe ****\"\n\nCAFFE_HOME=/usr/local/caffe\ngit clone https://github.com/BVLC/caffe.git $CAFFE_HOME\ncd $CAFFE_HOME\ngit checkout {caffeGitTag}\n\necho \"**** Installing Caffe Python dependencies ****\"\n\npip install -r python/requirements.txt\n\necho \"**** Installing Caffe ****\"\n\ncp Makefile.config.example Makefile.config\necho \"\nINCLUDE_DIRS += /usr/include/hdf5/serial\nLIBRARY_DIRS += /usr/lib/x86_64-linux-gnu/hdf5/serial\nBLAS := open\nUSE_CUDNN := 1\n\" >> Makefile.config\n\nmake all -j$(nproc)\nmake pycaffe\n\n# set up symlink\nln -s $CAFFE_HOME/python/caffe /usr/local/lib/python2.7/site-packages/\n\n\"\"\".format(caffeGitTag = caffeGitTag)"],"metadata":{},"outputs":[],"execution_count":121},{"cell_type":"code","source":["dbutils.fs.put(\"dbfs:/databricks/init/%s/install-caffe-gpu.sh\" % clusterName, script, True)"],"metadata":{},"outputs":[],"execution_count":122},{"cell_type":"code","source":["print(\"Init scripts installed for cluster %s:\" % clusterName)\nprint(\"\\n\".join([x.name for x in dbutils.fs.ls(\"dbfs:/databricks/init/%s\" % clusterName)]))"],"metadata":{},"outputs":[],"execution_count":123},{"cell_type":"code","source":["from pylab import *"],"metadata":{},"outputs":[],"execution_count":124},{"cell_type":"code","source":["import caffe"],"metadata":{},"outputs":[],"execution_count":125},{"cell_type":"code","source":["import os\ncaffe_root = \"/usr/local/caffe\"\nos.chdir(caffe_root)"],"metadata":{},"outputs":[],"execution_count":126},{"cell_type":"code","source":["%sh # run scripts from caffe root\n\n# Download data\ndata/mnist/get_mnist.sh\n\n# Prepare data\nexamples/mnist/create_mnist.sh"],"metadata":{},"outputs":[],"execution_count":127},{"cell_type":"code","source":["# back to examples\nos.chdir('examples')"],"metadata":{},"outputs":[],"execution_count":128},{"cell_type":"code","source":["from caffe import layers as L, params as P\n\ndef lenet(lmdb, batch_size):\n    # our version of LeNet: a series of linear and simple nonlinear transformations\n    n = caffe.NetSpec()\n    \n    n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb,\n                             transform_param=dict(scale=1./255), ntop=2)\n    \n    n.conv1 = L.Convolution(n.data, kernel_size=5, num_output=20, weight_filler=dict(type='xavier'))\n    n.pool1 = L.Pooling(n.conv1, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n    n.conv2 = L.Convolution(n.pool1, kernel_size=5, num_output=50, weight_filler=dict(type='xavier'))\n    n.pool2 = L.Pooling(n.conv2, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n    n.fc1 =   L.InnerProduct(n.pool2, num_output=500, weight_filler=dict(type='xavier'))\n    n.relu1 = L.ReLU(n.fc1, in_place=True)\n    n.score = L.InnerProduct(n.relu1, num_output=10, weight_filler=dict(type='xavier'))\n    n.loss =  L.SoftmaxWithLoss(n.score, n.label)\n    \n    return n.to_proto()\n    \nwith open('mnist/lenet_auto_train.prototxt', 'w') as f:\n    f.write(str(lenet('mnist/mnist_train_lmdb', 64)))\n    \nwith open('mnist/lenet_auto_test.prototxt', 'w') as f:\n    f.write(str(lenet('mnist/mnist_test_lmdb', 100)))"],"metadata":{},"outputs":[],"execution_count":129},{"cell_type":"code","source":["%sh cat mnist/lenet_auto_train.prototxt"],"metadata":{},"outputs":[],"execution_count":130},{"cell_type":"code","source":["%sh cat mnist/lenet_auto_solver.prototxt"],"metadata":{},"outputs":[],"execution_count":131},{"cell_type":"code","source":["caffe.set_device(0)\ncaffe.set_mode_gpu()\n\n### load the solver and create train and test nets\nsolver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\nsolver = caffe.SGDSolver('mnist/lenet_auto_solver.prototxt')"],"metadata":{},"outputs":[],"execution_count":132},{"cell_type":"code","source":["# each output is (batch size, feature dim, spatial dim)\n[(k, v.data.shape) for k, v in solver.net.blobs.items()]"],"metadata":{},"outputs":[],"execution_count":133},{"cell_type":"code","source":["# just print the weight sizes (we'll omit the biases)\n[(k, v[0].data.shape) for k, v in solver.net.params.items()]"],"metadata":{},"outputs":[],"execution_count":134},{"cell_type":"code","source":["solver.net.forward()  # train net\nsolver.test_nets[0].forward()  # test net (there can be more than one)"],"metadata":{},"outputs":[],"execution_count":135},{"cell_type":"code","source":["# we use a little trick to tile the first eight images\nclf()\nimshow(solver.net.blobs['data'].data[:8, 0].transpose(1, 0, 2).reshape(28, 8*28), cmap='gray'); axis('off')\ndisplay()\n\n"],"metadata":{},"outputs":[],"execution_count":136},{"cell_type":"code","source":["print 'train labels:', solver.net.blobs['label'].data[:8]"],"metadata":{},"outputs":[],"execution_count":137},{"cell_type":"code","source":["clf()\nimshow(solver.test_nets[0].blobs['data'].data[:8, 0].transpose(1, 0, 2).reshape(28, 8*28), cmap='gray'); axis('off')\ndisplay()"],"metadata":{},"outputs":[],"execution_count":138},{"cell_type":"code","source":["print 'test labels:', solver.test_nets[0].blobs['label'].data[:8]"],"metadata":{},"outputs":[],"execution_count":139},{"cell_type":"code","source":["cntkVersion = \"2.0.beta15.0\"\nuseGPU = True\nclusterName = \"cntk-gpu\""],"metadata":{},"outputs":[],"execution_count":140},{"cell_type":"code","source":["device = \"GPU\" if useGPU else \"CPU-only\""],"metadata":{},"outputs":[],"execution_count":141},{"cell_type":"code","source":["script = \"\"\"\n#!/usr/bin/env bash\nCNTK_VER=\"{cntkVersion}\"\nCNTK_URL=\"https://cntk.ai/PythonWheel/{device}/cntk-$CNTK_VER-cp27-cp27mu-linux_x86_64.whl\"\necho \"Running apt-get upgrade\"\napt-get upgrade\n\necho \"Installing Open MPI\"\napt-get -y install openmpi-bin\n\necho \"Installing CNTK via pip\"\npip install --upgrade $CNTK_URL\n\"\"\".format(cntkVersion=cntkVersion, device=device)"],"metadata":{},"outputs":[],"execution_count":142},{"cell_type":"code","source":["dbutils.fs.put(\"dbfs:/databricks/init/{clusterName}/install-cntk.sh\".format(clusterName=clusterName), script, True)"],"metadata":{},"outputs":[],"execution_count":143},{"cell_type":"code","source":["print \"Init scripts installed for cluster: \" + clusterName\nprint(\"\\n\".join([x.name for x in dbutils.fs.ls(\"dbfs:/databricks/init/%s\" % clusterName)]))"],"metadata":{},"outputs":[],"execution_count":144},{"cell_type":"code","source":["\nMODEL_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\nmodel_dir = '/tmp/imagenet'\n\nIMAGES_INDEX_URL = 'http://image-net.org/imagenet_data/urls/imagenet_fall11_urls.tgz'\nimages_read_limit = 1000L  # Increase this to read more images\n\n# Number of images per batch.\n# 1 batch corresponds to 1 RDD row.\nimage_batch_size = 3\n\nnum_top_predictions = 5"],"metadata":{},"outputs":[],"execution_count":145},{"cell_type":"code","source":["import numpy as np\nimport tensorflow as tf\nimport os\nfrom tensorflow.python.platform import gfile\nimport os.path\nimport re\nimport sys\nimport tarfile\nfrom subprocess import Popen, PIPE, STDOUT"],"metadata":{},"outputs":[],"execution_count":146},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":147}],"metadata":{"name":"databricks docs 9 (sparkR graphx","notebookId":3117922773650891},"nbformat":4,"nbformat_minor":0}
